{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f834506a-02ef-4e35-8f2f-cc7985a43f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyDTDM import *\n",
    "import warnings\n",
    "import yaml\n",
    "try:\n",
    "    from yaml import Cloader as Loader\n",
    "except ImportError:\n",
    "\n",
    "    from yaml import Loader\n",
    "\n",
    "import xarray as xr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f95d3525-27b4-482b-97fb-3beab889e37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n",
      " Parameters set from InputFiles/phase2NNR_paleotopography.yaml\n",
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n"
     ]
    }
   ],
   "source": [
    "# Define the path to the configuration file\n",
    "config_file = \"InputFiles/phase2NNR_paleotopography.yaml\"\n",
    "\n",
    "# Open the configuration file and load parameters using YAML\n",
    "with open(config_file) as f:\n",
    "    PARAMS = yaml.load(f, Loader=Loader)  # Load parameters from YAML file using the specified Loader\n",
    "\n",
    "# Print a confirmation message indicating the configuration file and parameters\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \")\n",
    "print(\" Parameters set from %s\" % config_file)  # Display the path of the configuration file\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c79e1195-9d59-46c9-adac-4b406e092f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n",
      "Reading input file..... \n",
      "\n",
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n",
      "Plate Model: phase2NNR \n",
      "\n",
      "Model Directory: /Users/ssin4735/Documents/PROJECT/PhD Project/Codes and Data/BhuDM/Zahirovic_etal_2022_GDJ_NNR \n",
      "\n",
      "Coastlines: /Users/ssin4735/Documents/PROJECT/PhD Project/Codes and Data/BhuDM/Zahirovic_etal_2022_GDJ_NNR/StaticGeometries/Coastlines/Global_coastlines_low_res.shp \n",
      "\n",
      "Continents: /Users/ssin4735/Documents/PROJECT/PhD Project/Codes and Data/BhuDM/Zahirovic_etal_2022_GDJ_NNR/StaticGeometries/ContinentalPolygons/Global_EarthByte_GPlates_PresentDay_ContinentalPolygons.shp \n",
      "\n",
      "Static Polygons: /Users/ssin4735/Documents/PROJECT/PhD Project/Codes and Data/BhuDM/Zahirovic_etal_2022_GDJ_NNR/StaticGeometries/StaticPolygons/Global_EarthByte_GPlates_PresentDay_StaticPlatePolygons.shp \n",
      "\n",
      "Model Agegrid: /Users/ssin4735/Documents/PROJECT/PhD Project/Codes and Data/BhuDM/SeafloorAgegrid \n",
      "\n",
      "ETopo grid: /Users/ssin4735/Documents/PROJECT/PhD Project/Codes and Data/Part 2/A_DEEPTIMETOPO/Data/Smoothed ETopo.tif\n",
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Input Files \n",
    "MODEL_NAME=PARAMS['InputFiles']['plate_kinematics']['model_name']\n",
    "MODEL_DIR = PARAMS['InputFiles']['plate_kinematics']['model_dir']  ## plate model\n",
    "topology_filenames =[f\"{MODEL_DIR}/{i}\" for i in PARAMS['InputFiles']['plate_kinematics']['topology_files']]\n",
    "rotation_filenames = [f\"{MODEL_DIR}/{i}\" for i in PARAMS['InputFiles']['plate_kinematics']['rotation_files']]\n",
    "agegrid=PARAMS['InputFiles']['plate_kinematics']['agegrid']\n",
    "\n",
    "ETOPO_FILE=PARAMS['InputFiles']['Raster']['ETOPO_FILE'] # ETOPO grid in meters (can be netCDf or GeoTiff)\n",
    "ETOPO_Type=PARAMS['InputFiles']['Raster']['Raster_type']\n",
    "coastlines = f\"{MODEL_DIR }/{PARAMS['InputFiles']['plate_kinematics']['coastline_file']}\"\n",
    "static_polygon_file=f\"{MODEL_DIR }/{PARAMS['InputFiles']['plate_kinematics']['static_polygon']}\"\n",
    "static_polygons = pygplates.FeatureCollection(static_polygon_file)\n",
    "continents=f\"{MODEL_DIR }/{PARAMS['InputFiles']['plate_kinematics']['continents']}\"\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \")\n",
    "print(\"Reading input file..... \\n\")\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \")\n",
    "print(f\"Plate Model: {MODEL_NAME} \\n\")\n",
    "print(f\"Model Directory: {MODEL_DIR} \\n\")\n",
    "print(f\"Coastlines: {coastlines} \\n\")\n",
    "print(f\"Continents: {continents} \\n\")\n",
    "print(f\"Static Polygons: {static_polygon_file} \\n\")\n",
    "print(f\"Model Agegrid: {agegrid} \\n\")\n",
    "print(f\"ETopo grid: {ETOPO_FILE}\")\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35a6fc3d-921f-4280-a147-a8e63c5dcc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n",
      "The following parameters are set-\n",
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n",
      "Mantle Optmised Reference Frame ID: 666666\n",
      "Paleomagnetic Reference Frame ID: 0 \n",
      "\n",
      "Moving Window Size: 25\n",
      "Weighted Mean: True\n",
      "Mesh Refinement Level: 9\n",
      "NetCDF GRID Resolution: 0.1\n",
      "NetCDF Compression Level: 5 \n",
      "\n",
      "Model Start Time: 10\n",
      "Model End Time: 0\n",
      "Model Time Step: 1\n",
      "\n",
      "Number of CPU: -1\n",
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n"
     ]
    }
   ],
   "source": [
    "Paleomag_ID=PARAMS['Parameters']['paleomag_id']\n",
    "Mantle_ID=PARAMS['Parameters']['mantle_optimised_id']\n",
    "\n",
    "#The initial positions of crustal points are evenly distributed within the designated region. \n",
    "# At mesh refinement level zero, the points are approximately 20 degrees apart.\n",
    "# Each increase in the density level results in a halving of the spacing between points.\n",
    "MESH_REFINEMENT_LEVEL=PARAMS['Parameters']['mesh_refinement_level']  # higher refinement level will take longer time to run for optimisation \n",
    "WINDOW_SIZE=int(PARAMS['Parameters']['time_window_size'])\n",
    "Weighted=PARAMS['Parameters']['weighted_mean']\n",
    "\n",
    "\n",
    "NETCDF_GRID_RESOLUTION=PARAMS['GridParameters']['grid_spacing']  # in degree\n",
    "ZLIB=PARAMS['GridParameters']['compression']['zlib'] \n",
    "COMPLEVEL=PARAMS['GridParameters']['compression']['complevel'] \n",
    "\n",
    "FROM_TIME=int(PARAMS['TimeParameters']['time_max'])\n",
    "TO_TIME=int(PARAMS['TimeParameters']['time_min'])\n",
    "TIME_STEPS=int(PARAMS['TimeParameters']['time_step'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parallel=PARAMS['Parameters']['number_of_cpus']### No of core to use or None for single core\n",
    "\n",
    "\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \")\n",
    "print(\"The following parameters are set-\")\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \")\n",
    "print(f\"Mantle Optmised Reference Frame ID: {Mantle_ID}\")\n",
    "print(f\"Paleomagnetic Reference Frame ID: {Paleomag_ID} \\n\")\n",
    "\n",
    "print(f\"Moving Window Size: {WINDOW_SIZE}\")\n",
    "print(f\"Weighted Mean: {Weighted}\")\n",
    "\n",
    "print(f\"Mesh Refinement Level: {MESH_REFINEMENT_LEVEL}\")\n",
    "print(f\"NetCDF GRID Resolution: {NETCDF_GRID_RESOLUTION}\")\n",
    "print(f\"NetCDF Compression Level: {COMPLEVEL} \\n\")\n",
    "print(f\"Model Start Time: {FROM_TIME}\")\n",
    "print(f\"Model End Time: {TO_TIME}\")\n",
    "print(f\"Model Time Step: {TIME_STEPS}\\n\")\n",
    "\n",
    "\n",
    "print(f\"Number of CPU: {parallel}\") # -1 means all the freely available CPU\n",
    "\n",
    "\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53aa0928-59f0-43c0-aae0-5411464fdb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n",
      "All the output will be saved in /Volumes/Satyam/BHPDryRun\n",
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n",
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n"
     ]
    }
   ],
   "source": [
    "# Output Directory\n",
    "OUTPUT_FOLDER=PARAMS['OutputFiles']['output_dir']\n",
    "\n",
    "DEFAULT_OUTPUT_CSV=os.path.join(OUTPUT_FOLDER,'CSV')\n",
    "DEFAULT_OUTPUT_NetCDF=os.path.join(OUTPUT_FOLDER,'NetCDF') # folder to store output NetCDF grid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \")\n",
    "print(f\"All the output will be saved in {OUTPUT_FOLDER}\")\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \")\n",
    "create_directory_if_not_exists(OUTPUT_FOLDER)\n",
    "create_directory_if_not_exists(DEFAULT_OUTPUT_CSV)\n",
    "create_directory_if_not_exists(DEFAULT_OUTPUT_NetCDF)\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e748df8-4075-4e5d-a847-9b226a5531fc",
   "metadata": {},
   "source": [
    "## Define Plate Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4178c7c8-161d-49a7-8f67-d86f9aa460a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ImportWarning: \n",
      "RotationModel: No filename associated with <class 'pygplates.pygplates.RotationModel'> in __init__\n",
      " ensure pygplates is imported from gplately. Run,\n",
      " from gplately import pygplates\n"
     ]
    }
   ],
   "source": [
    "# rotation_model = pygplates.RotationModel(rotation_filenames)\n",
    "# topology_features = pygplates.FeatureCollection()\n",
    "# for topology_filename in topology_filenames:\n",
    "#         topology_features.add( pygplates.FeatureCollection(topology_filename))\n",
    "\n",
    "\n",
    "PK=PlateKinematicsParameters(topology_filenames, \n",
    "                             rotation_filenames,\n",
    "                             static_polygons,\n",
    "                             agegrid=agegrid,\n",
    "                             coastlines=coastlines,\n",
    "                             continents=continents,\n",
    "                             anchor_plate_id=Mantle_ID)\n",
    "\n",
    "time = 0 #Ma\n",
    "gplot = gplately.PlotTopologies(PK.model, coastlines=coastlines, continents=continents, time=time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f250ae02-f2d4-4601-ae9b-09778f4c6298",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_times=glob.glob(f\"{DEFAULT_OUTPUT_CSV}/Prediction/*\")\n",
    "all_times=np.sort([int(time.split('_')[-1].split('.')[0].split('Ma')[0]) for time in all_times])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e0d7ec2-25dd-410d-8208-aa3245e097a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd041e50-6bda-43e1-89ad-e4d006fedeb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: /Volumes/Satyam/BHPDryRun/NetCDF/RF_Model\n",
      "Working on Time=0 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=1 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=2 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=3 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=4 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=5 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=6 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=7 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=8 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=9 Ma\n",
      "Saved NetCDF!\n"
     ]
    }
   ],
   "source": [
    "create_directory_if_not_exists(f\"{DEFAULT_OUTPUT_NetCDF}/RF_Model\")\n",
    "\n",
    "compression = {'zlib': ZLIB, 'complevel': COMPLEVEL}\n",
    "for reconstruction_time in all_times:\n",
    "\n",
    "    try:\n",
    "        print(f\"Working on Time={reconstruction_time} Ma\")\n",
    "        Data=pd.read_parquet(f'{DEFAULT_OUTPUT_CSV}/Prediction/Predicted_{MODEL_NAME}_{reconstruction_time}Ma.parquet')\n",
    "        column_for_netcdf1=\"ElevationRF\"\n",
    "    \n",
    "        if compression:\n",
    "            encoding = {column_for_netcdf1: compression}\n",
    "        else:\n",
    "            encoding = None\n",
    "        \n",
    "        da = df_to_NetCDF(x=Data['Longitude'], y=Data['Latitude'], z=Data[column_for_netcdf1], statistic='mean', grid_resolution=0.1, clip=(None, None))\n",
    "        ds=da.to_dataset(name=column_for_netcdf1)\n",
    "        ds.to_netcdf(f'{DEFAULT_OUTPUT_NetCDF}/RF_Model/RF_Model_{MODEL_NAME}_{reconstruction_time}.nc',encoding=encoding)\n",
    "        \n",
    "        print(\"Saved NetCDF!\")\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d4c49e-505a-4252-b9c4-acf514aa711a",
   "metadata": {},
   "source": [
    "# Post Processing Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cdc7d8d-3e8b-4933-a347-24f149ed07e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: /Volumes/Satyam/BHPDryRun/NetCDF/25_processed/RF_Model\n",
      "Post-processing grid =0 Ma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated 339054 points\n",
      "Saved!\n",
      "Post-processing grid =1 Ma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated 345244 points\n",
      "Saved!\n",
      "Post-processing grid =2 Ma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated 343579 points\n",
      "Saved!\n",
      "Post-processing grid =3 Ma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated 345666 points\n",
      "Saved!\n",
      "Post-processing grid =4 Ma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated 416360 points\n",
      "Saved!\n",
      "Post-processing grid =5 Ma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated 415774 points\n",
      "Saved!\n",
      "Post-processing grid =6 Ma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated 439126 points\n",
      "Saved!\n",
      "Post-processing grid =7 Ma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated 439049 points\n",
      "Saved!\n",
      "Post-processing grid =8 Ma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated 442074 points\n",
      "Saved!\n",
      "Post-processing grid =9 Ma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated 441795 points\n",
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# reconstruction_time=60\n",
    "create_directory_if_not_exists(f\"{DEFAULT_OUTPUT_NetCDF}/{WINDOW_SIZE}_processed/RF_Model\")\n",
    "\n",
    "\n",
    "for reconstruction_time in all_times:\n",
    "    try:\n",
    "        print(f\"Post-processing grid ={reconstruction_time} Ma\")\n",
    "        pygplates.reconstruct(continents,PK.rotation_model,\"tmp.shp\",reconstruction_time,anchor_plate_id=PK.anchor_plate_id)\n",
    "        recon_cgdf=gpd.read_file(\"tmp.shp\")\n",
    "        new_latitudes = np.arange(-90,90, NETCDF_GRID_RESOLUTION)\n",
    "        new_longitudes = np.arange(-180,180, NETCDF_GRID_RESOLUTION)\n",
    "        \n",
    "        \n",
    "        # Interpolate the data array along both latitude and longitude dimensions\n",
    "        # # Using 'linear' interpolation; other methods like 'nearest' are also available\n",
    "        # ds_interp = ds.interp(Latitude=new_latitudes, Longitude=new_longitudes, method='linear')\n",
    "        # ds_interp_copy = ds_copy.interp(Latitude=new_latitudes, Longitude=new_longitudes, method='linear')\n",
    "\n",
    "\n",
    "       \n",
    "        db = xr.open_dataset(f'{DEFAULT_OUTPUT_NetCDF}/RF_Model/RF_Model_{MODEL_NAME}_{reconstruction_time}.nc')\n",
    "        # Interpolate 'db' to match the grid of 'ds_interp'\n",
    "        # db_interp = db.interp(Latitude=new_latitudes, Longitude=new_longitudes, method='linear')\n",
    "        # db_interp_nearest = db_interp.interpolate_na(dim=['Latitude', 'Longitude'], method='nearest',fill_value=\"extrapolate\")\n",
    "        \n",
    "        db_interp=db\n",
    "        db_gdf=db_interp.to_dataframe().reset_index().dropna()\n",
    "        # db_gdf=db.to_dataframe().reset_index().dropna()\n",
    "        columns=['Latitude','Longitude','ElevationRF']\n",
    "    \n",
    "        all_points=db_interp.to_dataframe().reset_index()\n",
    "        # all_points=ds_interp.to_dataframe().reset_index()\n",
    "        all_nan = all_points[all_points['ElevationRF'].isna()]\n",
    "        all_elevation = all_points[all_points['ElevationRF'].isna()==False]\n",
    "        all_nan_gdf=gpd.GeoDataFrame(all_nan,geometry=gpd.points_from_xy(all_nan['Longitude'],all_nan['Latitude']))\n",
    "        all_nan_gdf=all_nan_gdf.set_crs(\"epsg:4326\")\n",
    "        all_within_continents=gpd.sjoin(all_nan_gdf,recon_cgdf,predicate='within',how='left')\n",
    "        all_within_continents=all_within_continents.dropna(subset=['ANCHOR', 'TIME', 'FILE1', 'RECONFILE1', 'PLATEID1', 'FROMAGE', 'TOAGE',\n",
    "               'NAME', 'PLATEID2', 'GPGIM_TYPE', 'L_PLATE', 'R_PLATE', 'SPREAD_ASY',\n",
    "               'IMPORT_AGE'])\n",
    "        \n",
    "        all_outside_continents = all_nan_gdf[~all_nan_gdf.index.isin(all_within_continents.index)]\n",
    "        all_outside_continents['ElevationRF']=-4000\n",
    "        all_within_continents['ElevationRF']=100\n",
    "        \n",
    "     \n",
    "        combined=pd.concat([all_outside_continents[columns],all_within_continents[columns],db_gdf[columns]])\n",
    "        # combined=pd.concat([all_outside_continents[columns],all_within_continents[columns]])#,db_gdf[columns]])\n",
    "        combined_gdf=gpd.GeoDataFrame(combined,geometry=gpd.points_from_xy(combined['Longitude'],combined['Latitude']))\n",
    "        \n",
    "        nan_cdf=df_to_NetCDF(combined['Longitude'],combined['Latitude'],combined['ElevationRF'])\n",
    "        \n",
    "         # Interpolating first along Latitude\n",
    "        \n",
    "        \n",
    "        # z_smooth2=gplately.grids.fill_raster(nan_cdf.values)\n",
    "        # z_smooth = nan_gaussian_filter(z_smooth2, sigma=4)\n",
    "        # interpolated_data_array = nan_cdf\n",
    "    \n",
    "        # cdf=post_process_grid(nan_cdf,\"Sa\",'z',threshold_distance=10,n_neighbors=5)\n",
    "    \n",
    "        \n",
    "        z_smooth2=gplately.grids.fill_raster(nan_cdf.values)\n",
    "        \n",
    "        z_smooth = nan_gaussian_filter(z_smooth2, sigma=4)\n",
    "        # z_smooth = nan_gaussian_filter(cdf.values, sigma=4)\n",
    "    # Create a new xarray Dataset with the smoothed data\n",
    "\n",
    "\n",
    "    \n",
    "        ds_smooth = xr.Dataset(\n",
    "            {\n",
    "                'ElevationRF': (('Latitude', 'Longitude'), z_smooth)\n",
    "            },\n",
    "            coords={\n",
    "                'Latitude': db_interp['Latitude'].values,\n",
    "                'Longitude': db_interp['Longitude'].values,\n",
    "            }\n",
    "        )\n",
    "        ds_smooth = ds_smooth.where(~db_interp.isnull(), np.nan)\n",
    "    \n",
    "        da_smooth=post_process_grid(ds_smooth['ElevationRF'],\"Sa\",'z',threshold_distance=5,n_neighbors=5)\n",
    "        \n",
    "        ds_smooth = xr.Dataset(\n",
    "            {\n",
    "                'ElevationRF': (('Latitude', 'Longitude'), da_smooth.values)\n",
    "            },\n",
    "            coords={\n",
    "                'Latitude': db_interp['Latitude'].values,\n",
    "                'Longitude': db_interp['Longitude'].values,\n",
    "            }\n",
    "        )   \n",
    "        # ds_smooth=ds_smooth.to_dataset(name='ElevationRF')\n",
    "    \n",
    "        # Define compression settings if needed\n",
    "        compression = {'zlib': True, 'complevel': 9}  # Adjust compression level as needed\n",
    "        \n",
    "        # Save the new dataset to a NetCDF file\n",
    "        output_file = f\"{DEFAULT_OUTPUT_NetCDF}/{WINDOW_SIZE}_processed/RF_Model/RF_Model_{MODEL_NAME}_{reconstruction_time}.nc\"\n",
    "        ds_smooth.to_netcdf(output_file, encoding={'ElevationRF': compression})\n",
    "        print(\"Saved!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping grid ={reconstruction_time} Ma\")\n",
    "        print(e)\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b82de6-5d6b-44c3-83e5-6f427c2c4f0f",
   "metadata": {},
   "source": [
    "# Change From Mantle Reference Frame to Paleomag Reference Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aca41a77-e425-453e-ba8a-d04311cb85c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotating grids:0 Ma\n",
      "Rotating grids:1 Ma\n",
      "Rotating grids:2 Ma\n",
      "Rotating grids:3 Ma\n",
      "Rotating grids:4 Ma\n",
      "Rotating grids:5 Ma\n",
      "Rotating grids:6 Ma\n",
      "Rotating grids:7 Ma\n",
      "Rotating grids:8 Ma\n",
      "Rotating grids:9 Ma\n"
     ]
    }
   ],
   "source": [
    "from gplately import Raster\n",
    "create_directory_if_not_exists(f\"{DEFAULT_OUTPUT_NetCDF}/{WINDOW_SIZE}_processed/RF_Model_Paleomag\")\n",
    "\n",
    "for reconstruction_time in all_times:\n",
    "    print(f\"Rotating grids:{reconstruction_time} Ma\")\n",
    "    da=xr.open_dataset(f'{DEFAULT_OUTPUT_NetCDF}/{WINDOW_SIZE}_processed/RF_Model/RF_Model_{MODEL_NAME}_{reconstruction_time}.nc')\n",
    "    raster=Raster(data=da.ElevationRF, plate_reconstruction=PK.model, extent='global',  time=reconstruction_time)\n",
    "    \n",
    "    raster.rotate_reference_frames(grid_spacing_degrees=NETCDF_GRID_RESOLUTION,\n",
    "                                   reconstruction_time=reconstruction_time, \n",
    "                                   from_rotation_features_or_model=PK.rotation_model, \n",
    "                                   to_rotation_features_or_model=PK.rotation_model, \n",
    "                                   from_rotation_reference_plate=Mantle_ID, \n",
    "                                   to_rotation_reference_plate=Paleomag_ID, \n",
    "                                   non_reference_plate=701, \n",
    "                                   output_name= f\"{DEFAULT_OUTPUT_NetCDF}/{WINDOW_SIZE}_processed/RF_Model_Paleomag/RF_Model_{MODEL_NAME}_{reconstruction_time}.nc\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
