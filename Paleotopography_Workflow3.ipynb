{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f834506a-02ef-4e35-8f2f-cc7985a43f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BhuDM import *\n",
    "from utils import *\n",
    "import warnings\n",
    "import yaml\n",
    "try:\n",
    "    from yaml import Cloader as Loader\n",
    "except ImportError:\n",
    "\n",
    "    from yaml import Loader\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, QuantileTransformer\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models, callbacks, optimizers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, QuantileTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f95d3525-27b4-482b-97fb-3beab889e37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n",
      " Parameters set from phase2_paleotopography.yaml\n",
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n"
     ]
    }
   ],
   "source": [
    "config_file=\"phase2_paleotopography.yaml\"\n",
    "with open(config_file) as f:\n",
    "    PARAMS = yaml.load(f, Loader=Loader)\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \")\n",
    "print(\" Parameters set from %s\" % config_file)\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c79e1195-9d59-46c9-adac-4b406e092f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n",
      "Reading input file..... \n",
      "\n",
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n",
      "Plate Model: phase2 \n",
      "\n",
      "Model Directory: /Users/ssin4735/Documents/PROJECT/PhD Project/Codes and Data/Phase2 \n",
      "\n",
      "Coastlines: /Users/ssin4735/Documents/PROJECT/PhD Project/Codes and Data/Phase2/StaticGeometries/Coastlines/Global_coastlines_low_res.shp \n",
      "\n",
      "Continents: /Users/ssin4735/Documents/PROJECT/PhD Project/Codes and Data/Phase2/StaticGeometries/ContinentalPolygons/Global_EarthByte_GPlates_PresentDay_ContinentsOnly.shp \n",
      "\n",
      "Static Polygons: /Users/ssin4735/Documents/PROJECT/PhD Project/Codes and Data/Phase2/StaticGeometries/StaticPolygons/Global_EarthByte_GPlates_PresentDay_StaticPlatePolygons.shp \n",
      "\n",
      "Model Agegrid: /Users/ssin4735/Documents/PROJECT/PhD Project/Codes and Data/Phase2/EarthByte_Plate_Motion_Model-Phase2-SeafloorAgeGrids-MantleFrame-NC \n",
      "\n",
      "ETopo grid: /Users/ssin4735/Documents/PROJECT/PhD Project/Codes and Data/Part 2/A_DEEPTIMETOPO/Data/Smoothed ETopo.tif\n",
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Input Files \n",
    "MODEL_NAME=PARAMS['InputFiles']['plate_kinematics']['model_name']\n",
    "MODEL_DIR = PARAMS['InputFiles']['plate_kinematics']['model_dir']  ## plate model\n",
    "topology_filenames =[f\"{MODEL_DIR}/{i}\" for i in PARAMS['InputFiles']['plate_kinematics']['topology_files']]\n",
    "rotation_filenames = [f\"{MODEL_DIR}/{i}\" for i in PARAMS['InputFiles']['plate_kinematics']['rotation_files']]\n",
    "agegrid=PARAMS['InputFiles']['plate_kinematics']['agegrid']\n",
    "\n",
    "ETOPO_FILE=PARAMS['InputFiles']['Raster']['ETOPO_FILE'] # ETOPO grid in meters (can be netCDf or GeoTiff)\n",
    "ETOPO_Type=PARAMS['InputFiles']['Raster']['Raster_type']\n",
    "coastlines = f\"{MODEL_DIR }/{PARAMS['InputFiles']['plate_kinematics']['coastline_file']}\"\n",
    "static_polygon_file=f\"{MODEL_DIR }/{PARAMS['InputFiles']['plate_kinematics']['static_polygon']}\"\n",
    "static_polygons = pygplates.FeatureCollection(static_polygon_file)\n",
    "continents=f\"{MODEL_DIR }/{PARAMS['InputFiles']['plate_kinematics']['continents']}\"\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \")\n",
    "print(\"Reading input file..... \\n\")\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \")\n",
    "print(f\"Plate Model: {MODEL_NAME} \\n\")\n",
    "print(f\"Model Directory: {MODEL_DIR} \\n\")\n",
    "print(f\"Coastlines: {coastlines} \\n\")\n",
    "print(f\"Continents: {continents} \\n\")\n",
    "print(f\"Static Polygons: {static_polygon_file} \\n\")\n",
    "print(f\"Model Agegrid: {agegrid} \\n\")\n",
    "print(f\"ETopo grid: {ETOPO_FILE}\")\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35a6fc3d-921f-4280-a147-a8e63c5dcc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n",
      "The following parameters are set-\n",
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n",
      "Mantle Optmised Reference Frame ID: 702702\n",
      "Paleomagnetic Reference Frame ID: 0 \n",
      "\n",
      "Moving Window Size: 15.0\n",
      "Weighted Mean: True\n",
      "Mesh Refinement Level: 9\n",
      "NetCDF GRID Resolution: 0.1\n",
      "NetCDF Compression Level: 5 \n",
      "\n",
      "Model Start Time: 525\n",
      "Model End Time: 0\n",
      "Model Time Step: 1\n",
      "\n",
      "Number of CPU: -1\n",
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n"
     ]
    }
   ],
   "source": [
    "Paleomag_ID=PARAMS['Parameters']['paleomag_id']\n",
    "Mantle_ID=PARAMS['Parameters']['mantle_optimised_id']\n",
    "\n",
    "#The initial positions of crustal points are evenly distributed within the designated region. \n",
    "# At mesh refinement level zero, the points are approximately 20 degrees apart.\n",
    "# Each increase in the density level results in a halving of the spacing between points.\n",
    "MESH_REFINEMENT_LEVEL=PARAMS['Parameters']['mesh_refinement_level']  # higher refinement level will take longer time to run for optimisation \n",
    "WINDOW_SIZE=PARAMS['Parameters']['time_window_size']\n",
    "Weighted=PARAMS['Parameters']['weighted_mean']\n",
    "\n",
    "\n",
    "NETCDF_GRID_RESOLUTION=PARAMS['GridParameters']['grid_spacing']  # in degree\n",
    "ZLIB=PARAMS['GridParameters']['compression']['zlib'] \n",
    "COMPLEVEL=PARAMS['GridParameters']['compression']['complevel'] \n",
    "\n",
    "FROM_TIME=int(PARAMS['TimeParameters']['time_max'])\n",
    "TO_TIME=int(PARAMS['TimeParameters']['time_min'])\n",
    "TIME_STEPS=int(PARAMS['TimeParameters']['time_step'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parallel=PARAMS['Parameters']['number_of_cpus']### No of core to use or None for single core\n",
    "\n",
    "\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \")\n",
    "print(\"The following parameters are set-\")\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \")\n",
    "print(f\"Mantle Optmised Reference Frame ID: {Mantle_ID}\")\n",
    "print(f\"Paleomagnetic Reference Frame ID: {Paleomag_ID} \\n\")\n",
    "\n",
    "print(f\"Moving Window Size: {WINDOW_SIZE}\")\n",
    "print(f\"Weighted Mean: {Weighted}\")\n",
    "\n",
    "print(f\"Mesh Refinement Level: {MESH_REFINEMENT_LEVEL}\")\n",
    "print(f\"NetCDF GRID Resolution: {NETCDF_GRID_RESOLUTION}\")\n",
    "print(f\"NetCDF Compression Level: {COMPLEVEL} \\n\")\n",
    "print(f\"Model Start Time: {FROM_TIME}\")\n",
    "print(f\"Model End Time: {TO_TIME}\")\n",
    "print(f\"Model Time Step: {TIME_STEPS}\\n\")\n",
    "\n",
    "\n",
    "print(f\"Number of CPU: {parallel}\") # -1 means all the freely available CPU\n",
    "\n",
    "\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53aa0928-59f0-43c0-aae0-5411464fdb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n",
      "All the output will be saved in Paleotopography\n",
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n",
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \n"
     ]
    }
   ],
   "source": [
    "# Output Directory\n",
    "OUTPUT_FOLDER=PARAMS['OutputFiles']['output_dir']\n",
    "\n",
    "DEFAULT_OUTPUT_CSV=os.path.join(OUTPUT_FOLDER,'CSV')\n",
    "DEFAULT_OUTPUT_NetCDF=os.path.join(OUTPUT_FOLDER,'NetCDF') # folder to store output NetCDF grid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \")\n",
    "print(f\"All the output will be saved in {OUTPUT_FOLDER}\")\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \")\n",
    "create_directory_if_not_exists(OUTPUT_FOLDER)\n",
    "create_directory_if_not_exists(DEFAULT_OUTPUT_CSV)\n",
    "create_directory_if_not_exists(DEFAULT_OUTPUT_NetCDF)\n",
    "print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e748df8-4075-4e5d-a847-9b226a5531fc",
   "metadata": {},
   "source": [
    "## Define Plate Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4178c7c8-161d-49a7-8f67-d86f9aa460a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ImportWarning: \n",
      "RotationModel: No filename associated with <class 'pygplates.pygplates.RotationModel'> in __init__\n",
      " ensure pygplates is imported from gplately. Run,\n",
      " from gplately import pygplates\n"
     ]
    }
   ],
   "source": [
    "# rotation_model = pygplates.RotationModel(rotation_filenames)\n",
    "# topology_features = pygplates.FeatureCollection()\n",
    "# for topology_filename in topology_filenames:\n",
    "#         topology_features.add( pygplates.FeatureCollection(topology_filename))\n",
    "\n",
    "\n",
    "PK=PlateKinematicsParameters(topology_filenames, \n",
    "                             rotation_filenames,\n",
    "                             static_polygons,\n",
    "                             agegrid=agegrid,\n",
    "                             coastlines=coastlines,\n",
    "                             continents=continents,\n",
    "                             anchor_plate_id=Mantle_ID)\n",
    "\n",
    "time = 0 #Ma\n",
    "gplot = gplately.PlotTopologies(PK.model, coastlines=coastlines, continents=continents, time=time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f250ae02-f2d4-4601-ae9b-09778f4c6298",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_times=glob.glob(f\"{DEFAULT_OUTPUT_CSV}/Prediction/*\")\n",
    "all_times=np.sort([int(time.split('_')[-1].split('.')[0].split('Ma')[0]) for time in all_times])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e0d7ec2-25dd-410d-8208-aa3245e097a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  86,  87,  88,  89,  90,  91,\n",
       "        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n",
       "       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n",
       "       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
       "       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
       "       144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
       "       157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "       170, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
       "       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196,\n",
       "       197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
       "       210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222,\n",
       "       223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235,\n",
       "       236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248,\n",
       "       249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261,\n",
       "       262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274,\n",
       "       275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287,\n",
       "       288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300,\n",
       "       301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313,\n",
       "       314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326,\n",
       "       327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
       "       340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352,\n",
       "       353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365,\n",
       "       366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378,\n",
       "       379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
       "       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n",
       "       405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417,\n",
       "       418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430,\n",
       "       432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444,\n",
       "       445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457,\n",
       "       458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470,\n",
       "       471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483,\n",
       "       484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496,\n",
       "       497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
       "       510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522,\n",
       "       523, 524])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "064d13be-0ace-497d-9ff7-042ab309e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_sz_df=pd.read_parquet(f\"{DEFAULT_OUTPUT_CSV}/ALL_Subduction.paraquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c881c005-8f7a-4b96-8f04-3f28f719dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_around_sub(i, subduction_df, n_steps=5,resolution=0.1):\n",
    "    results = {\n",
    "        'polygon':[],\n",
    "        'dist':[],\n",
    "        # 'trench_lats':[],\n",
    "#         'trench_lons':[]\n",
    "    }\n",
    "   \n",
    "    y1 = subduction_df.iloc[i]['Trench Latitude']\n",
    "    y2 = subduction_df.iloc[i + 1]['Trench Latitude']\n",
    "    x1 = subduction_df.iloc[i]['Trench Longitude']\n",
    "    x2 = subduction_df.iloc[i + 1]['Trench Longitude']\n",
    "\n",
    "    # dist = calc_dist(x1, y1, x2, y2)\n",
    "    \n",
    "    Point1=(x1,y1)\n",
    "    Point2=(x2,y2)\n",
    "    lines = LineString([Point1, Point2])\n",
    "    dist=lines.length\n",
    "\n",
    "    results['dist'].append(dist)\n",
    "\n",
    "    # if  subduction_df.iloc[i]['Trench Plate ID']== subduction_df.iloc[i+1]['Trench Plate ID'] and dist <= 5.0:\n",
    "    if  dist <= 5.0:\n",
    "        try:\n",
    "\n",
    "            ilon1=subduction_df.iloc[i]['Intersection Longitude'] \n",
    "            ilat1=subduction_df.iloc[i]['Intersection Latitude'] \n",
    "    \n",
    "            ilon2=subduction_df.iloc[i+1]['Intersection Longitude'] \n",
    "            ilat2=subduction_df.iloc[i+1]['Intersection Latitude'] \n",
    "            \n",
    "            coords = ((x1, y1), (x2, y2), (ilon2, ilat2), (ilon1, ilat1), (x1, y1))\n",
    "            polygon = Polygon(coords)\n",
    "            results['polygon'].append(polygon)\n",
    "            # _, lats, lons = multipoints_from_polygon(polygon, resolution=0.09)\n",
    "            # results['point_lats']=lats\n",
    "            # results['point_lons']=lons\n",
    "\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass        \n",
    "        \n",
    "        return results\n",
    "       \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd041e50-6bda-43e1-89ad-e4d006fedeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: Paleotopography/NetCDF/RF_Model\n",
      "Created directory: Paleotopography/NetCDF/DL_Model\n",
      "Created directory: Paleotopography/NetCDF/DLC_Model\n",
      "Created directory: Paleotopography/NetCDF/Ensemble_Model\n",
      "Created directory: Paleotopography/NetCDF/EBM_Model\n",
      "Working on Time=0 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=1 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=2 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=3 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=4 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=5 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=6 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=7 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=8 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=9 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=10 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=11 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=12 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=13 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=14 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=15 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=16 Ma\n",
      "Saved NetCDF!\n",
      "Working on Time=17 Ma\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m da \u001b[38;5;241m=\u001b[39m df_to_NetCDF(x\u001b[38;5;241m=\u001b[39mData[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLongitude\u001b[39m\u001b[38;5;124m'\u001b[39m], y\u001b[38;5;241m=\u001b[39mData[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m'\u001b[39m], z\u001b[38;5;241m=\u001b[39mData[column_for_netcdf2], statistic\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, grid_resolution\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, clip\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m     43\u001b[0m ds\u001b[38;5;241m=\u001b[39mda\u001b[38;5;241m.\u001b[39mto_dataset(name\u001b[38;5;241m=\u001b[39mcolumn_for_netcdf2)\n\u001b[0;32m---> 44\u001b[0m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDEFAULT_OUTPUT_NetCDF\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/EBM_Model/EBM_Model_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMODEL_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mreconstruction_time\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.nc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m column_for_netcdf2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElevationDL\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n",
      "File \u001b[0;32m~/miniconda3/envs/pygplates/lib/python3.10/site-packages/xarray/core/dataset.py:2298\u001b[0m, in \u001b[0;36mDataset.to_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[1;32m   2295\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2296\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_netcdf\n\u001b[0;32m-> 2298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore  # mypy cannot resolve the overloads:(\u001b[39;49;00m\n\u001b[1;32m   2299\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2302\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2306\u001b[0m \u001b[43m    \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlimited_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultifile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2309\u001b[0m \u001b[43m    \u001b[49m\u001b[43minvalid_netcdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvalid_netcdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2310\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pygplates/lib/python3.10/site-packages/xarray/backends/api.py:1339\u001b[0m, in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;66;03m# TODO: figure out how to refactor this logic (here and in save_mfdataset)\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;66;03m# to avoid this mess of conditionals\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1337\u001b[0m     \u001b[38;5;66;03m# TODO: allow this work (setting up the file for writing array data)\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;66;03m# to be parallelized with dask\u001b[39;00m\n\u001b[0;32m-> 1339\u001b[0m     \u001b[43mdump_to_store\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlimited_dims\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m autoclose:\n\u001b[1;32m   1343\u001b[0m         store\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/pygplates/lib/python3.10/site-packages/xarray/backends/api.py:1386\u001b[0m, in \u001b[0;36mdump_to_store\u001b[0;34m(dataset, store, writer, encoder, encoding, unlimited_dims)\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder:\n\u001b[1;32m   1384\u001b[0m     variables, attrs \u001b[38;5;241m=\u001b[39m encoder(variables, attrs)\n\u001b[0;32m-> 1386\u001b[0m \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlimited_dims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pygplates/lib/python3.10/site-packages/xarray/backends/common.py:397\u001b[0m, in \u001b[0;36mAbstractWritableDataStore.store\u001b[0;34m(self, variables, attributes, check_encoding_set, writer, unlimited_dims)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_attributes(attributes)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_dimensions(variables, unlimited_dims\u001b[38;5;241m=\u001b[39munlimited_dims)\n\u001b[0;32m--> 397\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_variables\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_encoding_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlimited_dims\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pygplates/lib/python3.10/site-packages/xarray/backends/common.py:439\u001b[0m, in \u001b[0;36mAbstractWritableDataStore.set_variables\u001b[0;34m(self, variables, check_encoding_set, writer, unlimited_dims)\u001b[0m\n\u001b[1;32m    434\u001b[0m check \u001b[38;5;241m=\u001b[39m vn \u001b[38;5;129;01min\u001b[39;00m check_encoding_set\n\u001b[1;32m    435\u001b[0m target, source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_variable(\n\u001b[1;32m    436\u001b[0m     name, v, check, unlimited_dims\u001b[38;5;241m=\u001b[39munlimited_dims\n\u001b[1;32m    437\u001b[0m )\n\u001b[0;32m--> 439\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pygplates/lib/python3.10/site-packages/xarray/backends/common.py:284\u001b[0m, in \u001b[0;36mArrayWriter.add\u001b[0;34m(self, source, target, region)\u001b[0m\n\u001b[1;32m    282\u001b[0m     target[region] \u001b[38;5;241m=\u001b[39m source\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[43mtarget\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m source\n",
      "File \u001b[0;32m~/miniconda3/envs/pygplates/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:81\u001b[0m, in \u001b[0;36mBaseNetCDF4Array.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     79\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_array(needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     80\u001b[0m data[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatastore\u001b[38;5;241m.\u001b[39mautoclose:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatastore\u001b[38;5;241m.\u001b[39mclose(needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "create_directory_if_not_exists(f\"{DEFAULT_OUTPUT_NetCDF}/RF_Model\")\n",
    "create_directory_if_not_exists(f\"{DEFAULT_OUTPUT_NetCDF}/DL_Model\")\n",
    "create_directory_if_not_exists(f\"{DEFAULT_OUTPUT_NetCDF}/DLC_Model\")\n",
    "create_directory_if_not_exists(f\"{DEFAULT_OUTPUT_NetCDF}/Ensemble_Model\")\n",
    "create_directory_if_not_exists(f\"{DEFAULT_OUTPUT_NetCDF}/EBM_Model\")\n",
    "compression = {'zlib': ZLIB, 'complevel': COMPLEVEL}\n",
    "for reconstruction_time in all_times:\n",
    "    if reconstruction_time>=100:\n",
    "        continue\n",
    "    # try:\n",
    "    print(f\"Working on Time={reconstruction_time} Ma\")\n",
    "    Data=pd.read_parquet(f'{DEFAULT_OUTPUT_CSV}/Prediction/Predicted_{MODEL_NAME}_{reconstruction_time}Ma.parquet')\n",
    "    column_for_netcdf1=\"ElevationRF\"\n",
    "\n",
    "    if compression:\n",
    "        encoding = {column_for_netcdf1: compression}\n",
    "    else:\n",
    "        encoding = None\n",
    "    \n",
    "    da = df_to_NetCDF(x=Data['Longitude'], y=Data['Latitude'], z=Data[column_for_netcdf1], statistic='mean', grid_resolution=0.1, clip=(None, None))\n",
    "    ds=da.to_dataset(name=column_for_netcdf1)\n",
    "    ds.to_netcdf(f'{DEFAULT_OUTPUT_NetCDF}/RF_Model/RF_Model_{MODEL_NAME}_{reconstruction_time}.nc',encoding=encoding)\n",
    "    \n",
    "    column_for_netcdf2=\"ElevationDLC\"\n",
    "   \n",
    "    if compression:\n",
    "        encoding = {column_for_netcdf2: compression}\n",
    "    else:\n",
    "        encoding = None\n",
    "    \n",
    "    da = df_to_NetCDF(x=Data['Longitude'], y=Data['Latitude'], z=Data[column_for_netcdf2], statistic='mean', grid_resolution=0.1, clip=(None, None))\n",
    "    ds=da.to_dataset(name=column_for_netcdf2)\n",
    "    ds.to_netcdf(f'{DEFAULT_OUTPUT_NetCDF}/DLC_Model/DLC_Model_{MODEL_NAME}_{reconstruction_time}.nc',encoding=encoding)\n",
    "\n",
    "    column_for_netcdf2=\"ElevationEBM\"\n",
    "   \n",
    "    if compression:\n",
    "        encoding = {column_for_netcdf2: compression}\n",
    "    else:\n",
    "        encoding = None\n",
    "    \n",
    "    da = df_to_NetCDF(x=Data['Longitude'], y=Data['Latitude'], z=Data[column_for_netcdf2], statistic='mean', grid_resolution=0.1, clip=(None, None))\n",
    "    ds=da.to_dataset(name=column_for_netcdf2)\n",
    "    ds.to_netcdf(f'{DEFAULT_OUTPUT_NetCDF}/EBM_Model/EBM_Model_{MODEL_NAME}_{reconstruction_time}.nc',encoding=encoding)\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    column_for_netcdf2=\"ElevationDL\"\n",
    "   \n",
    "    if compression:\n",
    "        encoding = {column_for_netcdf2: compression}\n",
    "    else:\n",
    "        encoding = None\n",
    "    \n",
    "    da = df_to_NetCDF(x=Data['Longitude'], y=Data['Latitude'], z=Data[column_for_netcdf2], statistic='mean', grid_resolution=0.1, clip=(None, None))\n",
    "    ds=da.to_dataset(name=column_for_netcdf2)\n",
    "    ds.to_netcdf(f'{DEFAULT_OUTPUT_NetCDF}/DL_Model/DL_Model_{MODEL_NAME}_{reconstruction_time}.nc',encoding=encoding)\n",
    "\n",
    "    column_for_netcdf3='Paleotopography'\n",
    "   \n",
    "    if compression:\n",
    "        encoding = {column_for_netcdf3: compression}\n",
    "    else:\n",
    "        encoding = None\n",
    "    \n",
    "    da = df_to_NetCDF(x=Data['Longitude'], y=Data['Latitude'], z=Data[column_for_netcdf3], statistic='mean', grid_resolution=0.1, clip=(None, None))\n",
    "    ds=da.to_dataset(name=column_for_netcdf3)\n",
    "    ds.to_netcdf(f'{DEFAULT_OUTPUT_NetCDF}/Ensemble_Model/Ensemble_Model_{MODEL_NAME}_{reconstruction_time}.nc',encoding=encoding)\n",
    "\n",
    "    print(\"Saved NetCDF!\")\n",
    "    # except Exception as e:\n",
    "    #     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea3dd566-0154-495e-adae-63b74bbb74cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: Paleotopography/NetCDF/Unmasked/RF_Model\n",
      "Created directory: Paleotopography/NetCDF/Unmasked/DL_Model\n",
      "Created directory: Paleotopography/NetCDF/Unmasked/DLC_Model\n",
      "Created directory: Paleotopography/NetCDF/Unmasked/Ensemble_Model\n",
      "Created directory: Paleotopography/NetCDF/Unmasked/EBM_Model\n",
      "Post Processing Time 0 Ma\n",
      "Post Processing Time 1 Ma\n",
      "Post Processing Time 2 Ma\n",
      "Post Processing Time 3 Ma\n",
      "Post Processing Time 4 Ma\n",
      "Post Processing Time 5 Ma\n",
      "Post Processing Time 6 Ma\n",
      "Post Processing Time 7 Ma\n",
      "Post Processing Time 8 Ma\n",
      "Post Processing Time 9 Ma\n",
      "Post Processing Time 10 Ma\n",
      "Post Processing Time 11 Ma\n",
      "Post Processing Time 12 Ma\n",
      "Post Processing Time 13 Ma\n",
      "Post Processing Time 14 Ma\n",
      "Post Processing Time 15 Ma\n",
      "Post Processing Time 16 Ma\n",
      "Post Processing Time 17 Ma\n",
      "Post Processing Time 18 Ma\n",
      "Post Processing Time 19 Ma\n",
      "Post Processing Time 20 Ma\n",
      "Post Processing Time 21 Ma\n",
      "Post Processing Time 22 Ma\n",
      "Post Processing Time 23 Ma\n",
      "Post Processing Time 24 Ma\n",
      "Post Processing Time 25 Ma\n",
      "Post Processing Time 26 Ma\n",
      "Post Processing Time 27 Ma\n",
      "Post Processing Time 28 Ma\n",
      "Post Processing Time 29 Ma\n",
      "Post Processing Time 30 Ma\n",
      "Post Processing Time 31 Ma\n",
      "Post Processing Time 32 Ma\n",
      "Post Processing Time 33 Ma\n",
      "Post Processing Time 34 Ma\n",
      "Post Processing Time 35 Ma\n",
      "Post Processing Time 36 Ma\n",
      "Post Processing Time 37 Ma\n",
      "Post Processing Time 38 Ma\n",
      "Post Processing Time 39 Ma\n",
      "Post Processing Time 40 Ma\n",
      "Post Processing Time 41 Ma\n",
      "Post Processing Time 42 Ma\n",
      "Post Processing Time 43 Ma\n",
      "Post Processing Time 44 Ma\n",
      "Post Processing Time 45 Ma\n",
      "Post Processing Time 46 Ma\n",
      "Post Processing Time 47 Ma\n",
      "Post Processing Time 48 Ma\n",
      "Post Processing Time 49 Ma\n",
      "Post Processing Time 50 Ma\n",
      "Post Processing Time 51 Ma\n",
      "Post Processing Time 52 Ma\n",
      "Post Processing Time 53 Ma\n",
      "Post Processing Time 54 Ma\n",
      "Post Processing Time 55 Ma\n",
      "Post Processing Time 56 Ma\n",
      "Post Processing Time 57 Ma\n",
      "Post Processing Time 58 Ma\n",
      "Post Processing Time 59 Ma\n",
      "Post Processing Time 60 Ma\n",
      "Post Processing Time 61 Ma\n",
      "Post Processing Time 62 Ma\n",
      "Post Processing Time 63 Ma\n",
      "Post Processing Time 64 Ma\n",
      "Post Processing Time 65 Ma\n",
      "Post Processing Time 66 Ma\n",
      "Post Processing Time 67 Ma\n",
      "Post Processing Time 68 Ma\n",
      "Post Processing Time 69 Ma\n",
      "Post Processing Time 70 Ma\n",
      "Post Processing Time 71 Ma\n",
      "Post Processing Time 72 Ma\n",
      "Post Processing Time 73 Ma\n",
      "Post Processing Time 74 Ma\n",
      "Post Processing Time 75 Ma\n",
      "Post Processing Time 76 Ma\n",
      "Post Processing Time 77 Ma\n",
      "Post Processing Time 78 Ma\n",
      "Post Processing Time 79 Ma\n",
      "Post Processing Time 80 Ma\n",
      "Post Processing Time 81 Ma\n",
      "Post Processing Time 82 Ma\n",
      "Post Processing Time 83 Ma\n",
      "Post Processing Time 84 Ma\n",
      "Post Processing Time 86 Ma\n",
      "Post Processing Time 87 Ma\n",
      "Post Processing Time 88 Ma\n",
      "Post Processing Time 89 Ma\n",
      "Post Processing Time 90 Ma\n",
      "Post Processing Time 91 Ma\n",
      "Post Processing Time 92 Ma\n",
      "Post Processing Time 93 Ma\n",
      "Post Processing Time 94 Ma\n",
      "Post Processing Time 95 Ma\n",
      "Post Processing Time 96 Ma\n",
      "Post Processing Time 97 Ma\n",
      "Post Processing Time 98 Ma\n",
      "Post Processing Time 99 Ma\n",
      "Post Processing Time 100 Ma\n",
      "Post Processing Time 101 Ma\n",
      "Post Processing Time 102 Ma\n",
      "Post Processing Time 103 Ma\n",
      "Post Processing Time 104 Ma\n",
      "Post Processing Time 105 Ma\n",
      "Post Processing Time 106 Ma\n",
      "Post Processing Time 107 Ma\n",
      "Post Processing Time 108 Ma\n",
      "Post Processing Time 109 Ma\n",
      "Post Processing Time 110 Ma\n",
      "Post Processing Time 111 Ma\n",
      "Post Processing Time 112 Ma\n",
      "Post Processing Time 113 Ma\n",
      "Post Processing Time 114 Ma\n",
      "Post Processing Time 115 Ma\n",
      "Post Processing Time 116 Ma\n",
      "Post Processing Time 117 Ma\n",
      "Post Processing Time 118 Ma\n",
      "Post Processing Time 119 Ma\n",
      "Post Processing Time 120 Ma\n",
      "Post Processing Time 121 Ma\n",
      "Post Processing Time 122 Ma\n",
      "Post Processing Time 123 Ma\n",
      "Post Processing Time 124 Ma\n",
      "Post Processing Time 125 Ma\n",
      "Post Processing Time 126 Ma\n",
      "Post Processing Time 127 Ma\n",
      "Post Processing Time 128 Ma\n",
      "Post Processing Time 129 Ma\n",
      "Post Processing Time 130 Ma\n",
      "Post Processing Time 131 Ma\n",
      "Post Processing Time 132 Ma\n",
      "Post Processing Time 133 Ma\n",
      "Post Processing Time 134 Ma\n",
      "Post Processing Time 135 Ma\n",
      "Post Processing Time 136 Ma\n",
      "Post Processing Time 137 Ma\n",
      "Post Processing Time 138 Ma\n",
      "Post Processing Time 139 Ma\n",
      "Post Processing Time 140 Ma\n",
      "Post Processing Time 141 Ma\n",
      "Post Processing Time 142 Ma\n",
      "Post Processing Time 143 Ma\n",
      "Post Processing Time 144 Ma\n",
      "Post Processing Time 145 Ma\n",
      "Post Processing Time 146 Ma\n",
      "Post Processing Time 147 Ma\n",
      "Post Processing Time 148 Ma\n",
      "Post Processing Time 149 Ma\n",
      "Post Processing Time 150 Ma\n",
      "Post Processing Time 151 Ma\n",
      "Post Processing Time 152 Ma\n",
      "Post Processing Time 153 Ma\n",
      "Post Processing Time 154 Ma\n",
      "Post Processing Time 155 Ma\n",
      "Post Processing Time 156 Ma\n",
      "Post Processing Time 157 Ma\n",
      "Post Processing Time 158 Ma\n",
      "Post Processing Time 159 Ma\n",
      "Post Processing Time 160 Ma\n",
      "Post Processing Time 161 Ma\n",
      "Post Processing Time 162 Ma\n",
      "Post Processing Time 163 Ma\n",
      "Post Processing Time 164 Ma\n",
      "Post Processing Time 165 Ma\n",
      "Post Processing Time 166 Ma\n",
      "Post Processing Time 167 Ma\n",
      "Post Processing Time 168 Ma\n",
      "Post Processing Time 169 Ma\n",
      "Post Processing Time 170 Ma\n",
      "Post Processing Time 171 Ma\n",
      "Post Processing Time 172 Ma\n",
      "Post Processing Time 174 Ma\n",
      "Post Processing Time 175 Ma\n",
      "Post Processing Time 176 Ma\n",
      "Post Processing Time 177 Ma\n",
      "Post Processing Time 178 Ma\n",
      "Post Processing Time 179 Ma\n",
      "Post Processing Time 180 Ma\n",
      "Post Processing Time 181 Ma\n",
      "Post Processing Time 182 Ma\n",
      "Post Processing Time 183 Ma\n",
      "Post Processing Time 184 Ma\n",
      "Post Processing Time 185 Ma\n",
      "Post Processing Time 186 Ma\n",
      "Post Processing Time 187 Ma\n",
      "Post Processing Time 188 Ma\n",
      "Post Processing Time 189 Ma\n",
      "Post Processing Time 190 Ma\n",
      "Post Processing Time 191 Ma\n",
      "Post Processing Time 192 Ma\n",
      "Post Processing Time 193 Ma\n",
      "Post Processing Time 194 Ma\n",
      "Post Processing Time 195 Ma\n",
      "Post Processing Time 196 Ma\n",
      "Post Processing Time 197 Ma\n",
      "Post Processing Time 198 Ma\n",
      "Post Processing Time 199 Ma\n",
      "Post Processing Time 200 Ma\n",
      "Post Processing Time 201 Ma\n",
      "Post Processing Time 202 Ma\n",
      "Post Processing Time 203 Ma\n",
      "Post Processing Time 204 Ma\n",
      "Post Processing Time 205 Ma\n",
      "Post Processing Time 206 Ma\n",
      "Post Processing Time 207 Ma\n",
      "Post Processing Time 208 Ma\n",
      "Post Processing Time 209 Ma\n",
      "Post Processing Time 210 Ma\n",
      "Post Processing Time 211 Ma\n",
      "Post Processing Time 212 Ma\n",
      "Post Processing Time 213 Ma\n",
      "Post Processing Time 214 Ma\n",
      "Post Processing Time 215 Ma\n",
      "Post Processing Time 216 Ma\n",
      "Post Processing Time 217 Ma\n",
      "Post Processing Time 218 Ma\n",
      "Post Processing Time 219 Ma\n",
      "Post Processing Time 220 Ma\n",
      "Post Processing Time 221 Ma\n",
      "Post Processing Time 222 Ma\n",
      "Post Processing Time 223 Ma\n",
      "Post Processing Time 224 Ma\n",
      "Post Processing Time 225 Ma\n",
      "Post Processing Time 226 Ma\n",
      "Post Processing Time 227 Ma\n",
      "Post Processing Time 228 Ma\n",
      "Post Processing Time 229 Ma\n",
      "Post Processing Time 230 Ma\n",
      "Post Processing Time 231 Ma\n",
      "Post Processing Time 232 Ma\n",
      "Post Processing Time 233 Ma\n",
      "Post Processing Time 234 Ma\n",
      "Post Processing Time 235 Ma\n",
      "Post Processing Time 236 Ma\n",
      "Post Processing Time 237 Ma\n",
      "Post Processing Time 238 Ma\n",
      "Post Processing Time 239 Ma\n",
      "Post Processing Time 240 Ma\n",
      "Post Processing Time 241 Ma\n",
      "Post Processing Time 242 Ma\n",
      "Post Processing Time 243 Ma\n",
      "Post Processing Time 244 Ma\n",
      "Post Processing Time 245 Ma\n",
      "Post Processing Time 246 Ma\n",
      "Post Processing Time 247 Ma\n",
      "Post Processing Time 248 Ma\n",
      "Post Processing Time 249 Ma\n",
      "Post Processing Time 250 Ma\n",
      "Post Processing Time 251 Ma\n",
      "Post Processing Time 252 Ma\n",
      "Post Processing Time 253 Ma\n",
      "Post Processing Time 254 Ma\n",
      "Post Processing Time 255 Ma\n",
      "Post Processing Time 256 Ma\n",
      "Post Processing Time 257 Ma\n",
      "Post Processing Time 258 Ma\n",
      "Post Processing Time 259 Ma\n",
      "Post Processing Time 260 Ma\n",
      "Post Processing Time 261 Ma\n",
      "Post Processing Time 262 Ma\n",
      "Post Processing Time 263 Ma\n",
      "Post Processing Time 264 Ma\n",
      "Post Processing Time 265 Ma\n",
      "Post Processing Time 266 Ma\n",
      "Post Processing Time 267 Ma\n",
      "Post Processing Time 268 Ma\n",
      "Post Processing Time 269 Ma\n",
      "Post Processing Time 270 Ma\n",
      "Post Processing Time 271 Ma\n",
      "Post Processing Time 272 Ma\n",
      "Post Processing Time 273 Ma\n",
      "Post Processing Time 274 Ma\n",
      "Post Processing Time 275 Ma\n",
      "Post Processing Time 276 Ma\n",
      "Post Processing Time 277 Ma\n",
      "Post Processing Time 278 Ma\n",
      "Post Processing Time 279 Ma\n",
      "Post Processing Time 280 Ma\n",
      "Post Processing Time 281 Ma\n",
      "Post Processing Time 282 Ma\n",
      "Post Processing Time 283 Ma\n",
      "Post Processing Time 284 Ma\n",
      "Post Processing Time 285 Ma\n",
      "Post Processing Time 286 Ma\n",
      "Post Processing Time 287 Ma\n",
      "Post Processing Time 288 Ma\n",
      "Post Processing Time 289 Ma\n",
      "Post Processing Time 290 Ma\n",
      "Post Processing Time 291 Ma\n",
      "Post Processing Time 292 Ma\n",
      "Post Processing Time 293 Ma\n",
      "Post Processing Time 294 Ma\n",
      "Post Processing Time 295 Ma\n",
      "Post Processing Time 296 Ma\n",
      "Post Processing Time 297 Ma\n",
      "Post Processing Time 298 Ma\n",
      "Post Processing Time 299 Ma\n",
      "Post Processing Time 300 Ma\n",
      "Post Processing Time 301 Ma\n",
      "Post Processing Time 302 Ma\n",
      "Post Processing Time 303 Ma\n",
      "Post Processing Time 304 Ma\n",
      "Post Processing Time 305 Ma\n",
      "Post Processing Time 306 Ma\n",
      "Post Processing Time 307 Ma\n",
      "Post Processing Time 308 Ma\n",
      "Post Processing Time 309 Ma\n",
      "Post Processing Time 310 Ma\n",
      "Post Processing Time 311 Ma\n",
      "Post Processing Time 312 Ma\n",
      "Post Processing Time 313 Ma\n",
      "Post Processing Time 314 Ma\n",
      "Post Processing Time 315 Ma\n",
      "Post Processing Time 316 Ma\n",
      "Post Processing Time 317 Ma\n",
      "Post Processing Time 318 Ma\n",
      "Post Processing Time 319 Ma\n",
      "Post Processing Time 320 Ma\n",
      "Post Processing Time 321 Ma\n",
      "Post Processing Time 322 Ma\n",
      "Post Processing Time 323 Ma\n",
      "Post Processing Time 324 Ma\n",
      "Post Processing Time 325 Ma\n",
      "Post Processing Time 326 Ma\n",
      "Post Processing Time 327 Ma\n",
      "Post Processing Time 328 Ma\n",
      "Post Processing Time 329 Ma\n",
      "Post Processing Time 330 Ma\n",
      "Post Processing Time 331 Ma\n",
      "Post Processing Time 332 Ma\n",
      "Post Processing Time 333 Ma\n",
      "Post Processing Time 334 Ma\n",
      "Post Processing Time 335 Ma\n",
      "Post Processing Time 336 Ma\n",
      "Post Processing Time 337 Ma\n",
      "Post Processing Time 338 Ma\n",
      "Post Processing Time 339 Ma\n",
      "Post Processing Time 340 Ma\n",
      "Post Processing Time 341 Ma\n",
      "Post Processing Time 342 Ma\n",
      "Post Processing Time 343 Ma\n",
      "Post Processing Time 344 Ma\n",
      "Post Processing Time 345 Ma\n",
      "Post Processing Time 346 Ma\n",
      "Post Processing Time 347 Ma\n",
      "Post Processing Time 348 Ma\n",
      "Post Processing Time 349 Ma\n",
      "Post Processing Time 350 Ma\n",
      "Post Processing Time 351 Ma\n",
      "Post Processing Time 352 Ma\n",
      "Post Processing Time 353 Ma\n",
      "Post Processing Time 354 Ma\n",
      "Post Processing Time 355 Ma\n",
      "Post Processing Time 356 Ma\n",
      "Post Processing Time 357 Ma\n",
      "Post Processing Time 358 Ma\n",
      "Post Processing Time 359 Ma\n",
      "Post Processing Time 360 Ma\n",
      "Post Processing Time 361 Ma\n",
      "Post Processing Time 362 Ma\n",
      "Post Processing Time 363 Ma\n",
      "Post Processing Time 364 Ma\n",
      "Post Processing Time 365 Ma\n",
      "Post Processing Time 366 Ma\n",
      "Post Processing Time 367 Ma\n",
      "Post Processing Time 368 Ma\n",
      "Post Processing Time 369 Ma\n",
      "Post Processing Time 370 Ma\n",
      "Post Processing Time 371 Ma\n",
      "Post Processing Time 372 Ma\n",
      "Post Processing Time 373 Ma\n",
      "Post Processing Time 374 Ma\n",
      "Post Processing Time 375 Ma\n",
      "Post Processing Time 376 Ma\n",
      "Post Processing Time 377 Ma\n",
      "Post Processing Time 378 Ma\n",
      "Post Processing Time 379 Ma\n",
      "Post Processing Time 380 Ma\n",
      "Post Processing Time 381 Ma\n",
      "Post Processing Time 382 Ma\n",
      "Post Processing Time 383 Ma\n",
      "Post Processing Time 384 Ma\n",
      "Post Processing Time 385 Ma\n",
      "Post Processing Time 386 Ma\n",
      "Post Processing Time 387 Ma\n",
      "Post Processing Time 388 Ma\n",
      "Post Processing Time 389 Ma\n",
      "Post Processing Time 390 Ma\n",
      "Post Processing Time 391 Ma\n",
      "Post Processing Time 392 Ma\n",
      "Post Processing Time 393 Ma\n",
      "Post Processing Time 394 Ma\n",
      "Post Processing Time 395 Ma\n",
      "Post Processing Time 396 Ma\n",
      "Post Processing Time 397 Ma\n",
      "Post Processing Time 398 Ma\n",
      "Post Processing Time 399 Ma\n",
      "Post Processing Time 400 Ma\n",
      "Post Processing Time 401 Ma\n",
      "Post Processing Time 402 Ma\n",
      "Post Processing Time 403 Ma\n",
      "Post Processing Time 404 Ma\n",
      "Post Processing Time 405 Ma\n",
      "Post Processing Time 406 Ma\n",
      "Post Processing Time 407 Ma\n",
      "Post Processing Time 408 Ma\n",
      "Post Processing Time 409 Ma\n",
      "Post Processing Time 410 Ma\n",
      "Post Processing Time 411 Ma\n",
      "Post Processing Time 412 Ma\n",
      "Post Processing Time 413 Ma\n",
      "Post Processing Time 414 Ma\n",
      "Post Processing Time 415 Ma\n",
      "Post Processing Time 416 Ma\n",
      "Post Processing Time 417 Ma\n",
      "Post Processing Time 418 Ma\n",
      "Post Processing Time 419 Ma\n",
      "Post Processing Time 420 Ma\n",
      "Post Processing Time 421 Ma\n",
      "Post Processing Time 422 Ma\n",
      "Post Processing Time 423 Ma\n",
      "Post Processing Time 424 Ma\n",
      "Post Processing Time 425 Ma\n",
      "Post Processing Time 426 Ma\n",
      "Post Processing Time 427 Ma\n",
      "Post Processing Time 428 Ma\n",
      "Post Processing Time 429 Ma\n",
      "Post Processing Time 430 Ma\n",
      "Post Processing Time 432 Ma\n",
      "Post Processing Time 433 Ma\n",
      "Post Processing Time 434 Ma\n",
      "Post Processing Time 435 Ma\n",
      "Post Processing Time 436 Ma\n",
      "Post Processing Time 437 Ma\n",
      "Post Processing Time 438 Ma\n",
      "Post Processing Time 439 Ma\n",
      "Post Processing Time 440 Ma\n",
      "Post Processing Time 441 Ma\n",
      "Post Processing Time 442 Ma\n",
      "Post Processing Time 443 Ma\n",
      "Post Processing Time 444 Ma\n",
      "Post Processing Time 445 Ma\n",
      "Post Processing Time 446 Ma\n",
      "Post Processing Time 447 Ma\n",
      "Post Processing Time 448 Ma\n",
      "Post Processing Time 449 Ma\n",
      "Post Processing Time 450 Ma\n",
      "Post Processing Time 451 Ma\n",
      "Post Processing Time 452 Ma\n",
      "Post Processing Time 453 Ma\n",
      "Post Processing Time 454 Ma\n",
      "Post Processing Time 455 Ma\n",
      "Post Processing Time 456 Ma\n",
      "Post Processing Time 457 Ma\n",
      "Post Processing Time 458 Ma\n",
      "Post Processing Time 459 Ma\n",
      "Post Processing Time 460 Ma\n",
      "Post Processing Time 461 Ma\n",
      "Post Processing Time 462 Ma\n",
      "Post Processing Time 463 Ma\n",
      "Post Processing Time 464 Ma\n",
      "Post Processing Time 465 Ma\n",
      "Post Processing Time 466 Ma\n",
      "Post Processing Time 467 Ma\n",
      "Post Processing Time 468 Ma\n",
      "Post Processing Time 469 Ma\n",
      "Post Processing Time 470 Ma\n",
      "Post Processing Time 471 Ma\n",
      "Post Processing Time 472 Ma\n",
      "Post Processing Time 473 Ma\n",
      "Post Processing Time 474 Ma\n",
      "Post Processing Time 475 Ma\n",
      "Post Processing Time 476 Ma\n",
      "Post Processing Time 477 Ma\n",
      "Post Processing Time 478 Ma\n",
      "Post Processing Time 479 Ma\n",
      "Post Processing Time 480 Ma\n",
      "Post Processing Time 481 Ma\n",
      "Post Processing Time 482 Ma\n",
      "Post Processing Time 483 Ma\n",
      "Post Processing Time 484 Ma\n",
      "Post Processing Time 485 Ma\n",
      "Post Processing Time 486 Ma\n",
      "Post Processing Time 487 Ma\n",
      "Post Processing Time 488 Ma\n",
      "Post Processing Time 489 Ma\n",
      "Post Processing Time 490 Ma\n",
      "Post Processing Time 491 Ma\n",
      "Post Processing Time 492 Ma\n",
      "Post Processing Time 493 Ma\n",
      "Post Processing Time 494 Ma\n",
      "Post Processing Time 495 Ma\n",
      "Post Processing Time 496 Ma\n",
      "Post Processing Time 497 Ma\n",
      "Post Processing Time 498 Ma\n",
      "Post Processing Time 499 Ma\n",
      "Post Processing Time 500 Ma\n",
      "Post Processing Time 501 Ma\n",
      "Post Processing Time 502 Ma\n",
      "Post Processing Time 503 Ma\n",
      "Post Processing Time 504 Ma\n",
      "Post Processing Time 505 Ma\n",
      "Post Processing Time 506 Ma\n",
      "Post Processing Time 507 Ma\n",
      "Post Processing Time 508 Ma\n",
      "Post Processing Time 509 Ma\n",
      "Post Processing Time 510 Ma\n",
      "Post Processing Time 511 Ma\n",
      "Post Processing Time 512 Ma\n",
      "Post Processing Time 513 Ma\n",
      "Post Processing Time 514 Ma\n",
      "Post Processing Time 515 Ma\n",
      "Post Processing Time 516 Ma\n",
      "Post Processing Time 517 Ma\n",
      "Post Processing Time 518 Ma\n",
      "Post Processing Time 519 Ma\n",
      "Post Processing Time 520 Ma\n",
      "Post Processing Time 521 Ma\n",
      "Post Processing Time 522 Ma\n",
      "Post Processing Time 523 Ma\n",
      "Post Processing Time 524 Ma\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import LineString\n",
    "import time\n",
    "from gplately import Raster\n",
    "\n",
    "create_directory_if_not_exists(f\"{DEFAULT_OUTPUT_NetCDF}/Unmasked/RF_Model\")\n",
    "create_directory_if_not_exists(f\"{DEFAULT_OUTPUT_NetCDF}/Unmasked/DL_Model\")\n",
    "create_directory_if_not_exists(f\"{DEFAULT_OUTPUT_NetCDF}/Unmasked/DLC_Model\")\n",
    "create_directory_if_not_exists(f\"{DEFAULT_OUTPUT_NetCDF}/Unmasked/Ensemble_Model\")\n",
    "create_directory_if_not_exists(f\"{DEFAULT_OUTPUT_NetCDF}/Unmasked/EBM_Model\")\n",
    "n_steps=4\n",
    "\n",
    "for reconstruction_time in all_times:\n",
    "    # if reconstruction_time<8:\n",
    "    #     continue\n",
    "    print(f\"Post Processing Time {reconstruction_time} Ma\")\n",
    "    # subduction_df = PK.get_subductiondf(reconstruction_time,tessellation_threshold_deg=0.7)\n",
    "    # subduction_lines=[]\n",
    "    # normal_angle=[]\n",
    "    \n",
    "    # for i in range(0, len(subduction_df)-1):\n",
    "    #     Point1=(subduction_df.iloc[i]['Trench Longitude'],subduction_df.iloc[i]['Trench Latitude'])\n",
    "    #     Point2=(subduction_df.iloc[i+1]['Trench Longitude'],subduction_df.iloc[i+1]['Trench Latitude'])\n",
    "    #     lines = LineString([Point1, Point2])\n",
    "    #     dist=lines.length\n",
    "    #     # if  subduction_df.iloc[i]['Trench Plate ID']== subduction_df.iloc[i+1]['Trench Plate ID'] and dist<50.0:\n",
    "    #     if  dist<=5.0:\n",
    "    #         subduction_lines.append(lines)\n",
    "    #     # else:\n",
    "    #     #     subduction_lines.append(np.nan)\n",
    "            \n",
    "        \n",
    "    # profiles=[]\n",
    "    # trench_lats=[]\n",
    "    # trench_lons=[]\n",
    "\n",
    "    # for i in range(0, len(subduction_df)-1):\n",
    "    #     dlon1 = n_steps * np.sin(np.radians(subduction_df.iloc[i]['Subduction Normal Angle']))\n",
    "    #     dlat1 = n_steps * np.cos(np.radians(subduction_df.iloc[i]['Subduction Normal Angle']))\n",
    "    #     x1=subduction_df.iloc[i]['Trench Longitude']\n",
    "    #     y1=subduction_df.iloc[i]['Trench Latitude']\n",
    "    #     ilon1 = x1 - dlon1\n",
    "    #     ilat1 = y1- dlat1\n",
    "        \n",
    "        \n",
    "    #     start_point = Point(x1- 0.01*dlon1, y1- 0.01*dlat1)\n",
    "    #     end_point = Point(ilon1, ilat1)\n",
    "    #     profile = LineString([start_point, end_point])\n",
    "    #     profiles.append(profile)\n",
    "    #     trench_lats.append(y1)\n",
    "    #     trench_lons.append(x1) \n",
    "        \n",
    "    # intersected_points=[]\n",
    "    # intersected_profiles=[]\n",
    "    # intersected_trench_lats=[]\n",
    "    # intersected_trench_lons=[]\n",
    "    # for i in range(len(profiles)):\n",
    "    #     for subduction_line in subduction_lines:\n",
    "            # try:\n",
    "    #         intersection=profiles[i].intersection(subduction_line)\n",
    "    #         if intersection.geom_type == 'Point':\n",
    "    #             # print(profiles[i])\n",
    "    #             intersected_profiles.append(profiles[i])\n",
    "    #             intersected_trench_lats.append(trench_lats[i])\n",
    "    #             intersected_trench_lons.append(trench_lons[i])\n",
    "    #             intersected_points.append(intersection)\n",
    "    #             # intersections2.append(intersection)\n",
    "                \n",
    "    # Intersection_df=gpd.GeoDataFrame()\n",
    "    # Intersection_df['Trench Latitude I']=intersected_trench_lats\n",
    "    # Intersection_df['Trench Longitude I']=intersected_trench_lons\n",
    "    # Intersection_df['Intersection Point']=intersected_points\n",
    "    # Intersection_df['Intersection Profile']=intersected_profiles\n",
    "    # Intersection_df['Intersection Latitude']=Intersection_df['Intersection Point'].apply(lambda p: p.y)\n",
    "    # Intersection_df['Intersection Longitude']=Intersection_df['Intersection Point'].apply(lambda p: p.x)\n",
    "    # Intersection_df=Intersection_df.set_geometry(gpd.points_from_xy(Intersection_df['Trench Longitude I'],Intersection_df['Trench Latitude I']))\n",
    "    # # Perform the spatial join\n",
    "    # result = gpd.sjoin_nearest(subduction_df, Intersection_df, how='left', max_distance=1.0)\n",
    "    # result=result.reset_index()\n",
    "    # # Drop rows with the same index, keeping only the first occurrence\n",
    "    # result = result.drop_duplicates(subset=['index'], keep='first')\n",
    "    # result=result.set_index('index')\n",
    "    # # Alternatively, drop duplicates based on the index column\n",
    "    # # If you want to drop duplicates based on specific columns, replace 'index_left' with the appropriate column names\n",
    "    \n",
    "    # result ['Intersection Latitude'] = result ['Intersection Latitude'].fillna(result['Trench Latitude'] -n_steps*np.cos(np.radians(result['Subduction Normal Angle'])))\n",
    "    # result ['Intersection Longitude'] = result ['Intersection Longitude'].fillna(result['Trench Longitude'] -n_steps*np.sin(np.radians(result['Subduction Normal Angle'])))\n",
    "    \n",
    "    \n",
    "    # polygons=[]\n",
    "\n",
    "    # results = Parallel(n_jobs=-1)(\n",
    "    #     delayed(poly_around_sub)(i, result,resolution=0.1) for i in range(len(result) - 1)\n",
    "    # )\n",
    "    \n",
    "    # for i, res in enumerate(results):\n",
    "    #     try:\n",
    "    #         polygons.extend(res['polygon'])\n",
    "\n",
    "    #     except Exception as e:\n",
    "    #         # print(e)\n",
    "    #         pass\n",
    "    \n",
    "    Data=pd.read_parquet(f'{DEFAULT_OUTPUT_CSV}/Prediction/Predicted_{MODEL_NAME}_{reconstruction_time}Ma.parquet')\n",
    "    # age_grid_file = find_filename_with_number(PK.agegrid,reconstruction_time)\n",
    "    # print(age_grid_file)\n",
    "    # oceanic_crust = rasterio.open(age_grid_file)\n",
    "    # coordinates = [(x, y) for x, y in zip(Data['Longitude'].values, Data['Latitude'].values)]\n",
    "    # oceanic_crust_point = list(oceanic_crust.sample(coordinates))\n",
    "    # oceanic_crust_point=[oceanic_crust_point[i][0] for i in range(len(oceanic_crust_point))]\n",
    "    # Data['Oceanic Crust']=oceanic_crust_point\n",
    "    # Data = Data[pd.isna(Data['Oceanic Crust'])]\n",
    "    # Data_gdf=gpd.GeoDataFrame(Data,geometry=gpd.points_from_xy(Data['Longitude'],Data['Latitude']))\n",
    "    # Data_gdf=Data_gdf.set_crs(\"epsg:4326\")\n",
    "    # polygons_gdf=gpd.GeoDataFrame(geometry=polygons)\n",
    "    # polygons_gdf=polygons_gdf.set_crs(\"epsg:4326\")\n",
    "\n",
    "    # # Ensure the coordinate reference systems match\n",
    "    # if Data_gdf.crs != polygons_gdf.crs:\n",
    "    #     Data_gdf = Data_gdf.to_crs(polygons_gdf.crs)\n",
    "    # if 'index_right' in Data_gdf.columns:\n",
    "    #     Data_gdf=Data_gdf.drop(columns=['index_right'])\n",
    "    # # Perform spatial join to find points within polygons\n",
    "    # points_within_polygons = gpd.sjoin(Data_gdf, polygons_gdf, predicate='within')\n",
    "    \n",
    "    # # Remove points that are within polygons from the original points GeoDataFrame\n",
    "    # points_outside_polygons = Data_gdf.loc[~Data_gdf.index.isin(points_within_polygons.index)]\n",
    "    # Data= points_outside_polygons\n",
    "    \n",
    "    \n",
    "    \n",
    "    column_for_netcdf=\"ElevationRF\"\n",
    "    \n",
    "    da = df_to_NetCDF(x=Data['Longitude'], y=Data['Latitude'], z=Data[column_for_netcdf], statistic='mean', grid_resolution=0.1, clip=(None, None))\n",
    "    \n",
    "    raster=Raster(data=da, plate_reconstruction=PK.model, extent='global',  time=reconstruction_time)\n",
    "    raster.rotate_reference_frames(grid_spacing_degrees=NETCDF_GRID_RESOLUTION,\n",
    "                                   reconstruction_time=reconstruction_time, \n",
    "                                   from_rotation_features_or_model=PK.rotation_model, \n",
    "                                   to_rotation_features_or_model=PK.rotation_model, \n",
    "                                   from_rotation_reference_plate=Mantle_ID, \n",
    "                                   to_rotation_reference_plate=Paleomag_ID, \n",
    "                                   non_reference_plate=701, \n",
    "                                   output_name=f'{DEFAULT_OUTPUT_NetCDF}/Unmasked/RF_Model/RF_Model_{MODEL_NAME}_{reconstruction_time}.nc')\n",
    "\n",
    "\n",
    "\n",
    "    column_for_netcdf=\"ElevationDL\"\n",
    "    \n",
    "    da = df_to_NetCDF(x=Data['Longitude'], y=Data['Latitude'], z=Data[column_for_netcdf], statistic='mean', grid_resolution=0.1, clip=(None, None))\n",
    "    \n",
    "    raster=Raster(data=da, plate_reconstruction=PK.model, extent='global',  time=reconstruction_time)\n",
    "    raster.rotate_reference_frames(grid_spacing_degrees=NETCDF_GRID_RESOLUTION,\n",
    "                                   reconstruction_time=reconstruction_time, \n",
    "                                   from_rotation_features_or_model=PK.rotation_model, \n",
    "                                   to_rotation_features_or_model=PK.rotation_model, \n",
    "                                   from_rotation_reference_plate=Mantle_ID, \n",
    "                                   to_rotation_reference_plate=Paleomag_ID, \n",
    "                                   non_reference_plate=701, \n",
    "                                   output_name=f'{DEFAULT_OUTPUT_NetCDF}/Unmasked/DL_Model//DL_Model_{MODEL_NAME}_{reconstruction_time}.nc')\n",
    "\n",
    "\n",
    "    column_for_netcdf=\"ElevationDLC\"\n",
    "    \n",
    "    da = df_to_NetCDF(x=Data['Longitude'], y=Data['Latitude'], z=Data[column_for_netcdf], statistic='mean', grid_resolution=0.1, clip=(None, None))\n",
    "    \n",
    "    raster=Raster(data=da, plate_reconstruction=PK.model, extent='global',  time=reconstruction_time)\n",
    "    raster.rotate_reference_frames(grid_spacing_degrees=NETCDF_GRID_RESOLUTION,\n",
    "                                   reconstruction_time=reconstruction_time, \n",
    "                                   from_rotation_features_or_model=PK.rotation_model, \n",
    "                                   to_rotation_features_or_model=PK.rotation_model, \n",
    "                                   from_rotation_reference_plate=Mantle_ID, \n",
    "                                   to_rotation_reference_plate=Paleomag_ID, \n",
    "                                   non_reference_plate=701, \n",
    "                                   output_name=f'{DEFAULT_OUTPUT_NetCDF}/Unmasked/DLC_Model/DLC_Model_{MODEL_NAME}_{reconstruction_time}.nc')\n",
    "\n",
    "\n",
    "\n",
    "    column_for_netcdf=\"Paleotopography\"\n",
    "    \n",
    "    da = df_to_NetCDF(x=Data['Longitude'], y=Data['Latitude'], z=Data[column_for_netcdf], statistic='mean', grid_resolution=0.1, clip=(None, None))\n",
    "    \n",
    "    raster=Raster(data=da, plate_reconstruction=PK.model, extent='global',  time=reconstruction_time)\n",
    "    raster.rotate_reference_frames(grid_spacing_degrees=NETCDF_GRID_RESOLUTION,\n",
    "                                   reconstruction_time=reconstruction_time, \n",
    "                                   from_rotation_features_or_model=PK.rotation_model, \n",
    "                                   to_rotation_features_or_model=PK.rotation_model, \n",
    "                                   from_rotation_reference_plate=Mantle_ID, \n",
    "                                   to_rotation_reference_plate=Paleomag_ID, \n",
    "                                   non_reference_plate=701, \n",
    "                                   output_name=f'{DEFAULT_OUTPUT_NetCDF}/Unmasked/Ensemble_Model/Ensemble_Model_{MODEL_NAME}_{reconstruction_time}.nc')\n",
    "\n",
    "\n",
    "    column_for_netcdf2=\"ElevationEBM\"\n",
    "   \n",
    "    if compression:\n",
    "        encoding = {column_for_netcdf2: compression}\n",
    "    else:\n",
    "        encoding = None\n",
    "    \n",
    "    da = df_to_NetCDF(x=Data['Longitude'], y=Data['Latitude'], z=Data[column_for_netcdf2], statistic='mean', grid_resolution=0.1, clip=(None, None))\n",
    "    raster=Raster(data=da, plate_reconstruction=PK.model, extent='global',  time=reconstruction_time)\n",
    "    raster.rotate_reference_frames(grid_spacing_degrees=NETCDF_GRID_RESOLUTION,\n",
    "                                   reconstruction_time=reconstruction_time, \n",
    "                                   from_rotation_features_or_model=PK.rotation_model, \n",
    "                                   to_rotation_features_or_model=PK.rotation_model, \n",
    "                                   from_rotation_reference_plate=Mantle_ID, \n",
    "                                   to_rotation_reference_plate=Paleomag_ID, \n",
    "                                   non_reference_plate=701, \n",
    "                                   output_name=f'{DEFAULT_OUTPUT_NetCDF}/Unmasked/EBM_Model/EBM_Model_{MODEL_NAME}_{reconstruction_time}.nc')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7394f6b5-469c-4d9f-b900-728b5593a1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Processing Time 0 Ma\n",
      "/Users/ssin4735/Documents/PROJECT/PhD Project/Codes and Data/Phase2/EarthByte_Plate_Motion_Model-Phase2-SeafloorAgeGrids-MantleFrame-NC/EarthByte_Plate_Motion_Model-Phase2-MantleReferenceFrame-0.nc\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import LineString\n",
    "import time\n",
    "from gplately import Raster\n",
    "\n",
    "create_directory_if_not_exists(f\"{DEFAULT_OUTPUT_NetCDF}/Masked/RF_Model\")\n",
    "create_directory_if_not_exists(f\"{DEFAULT_OUTPUT_NetCDF}/Masked/DL_Model\")\n",
    "create_directory_if_not_exists(f\"{DEFAULT_OUTPUT_NetCDF}/Masked/DLC_Model\")\n",
    "create_directory_if_not_exists(f\"{DEFAULT_OUTPUT_NetCDF}/Masked/Ensemble_Model\")\n",
    "create_directory_if_not_exists(f\"{DEFAULT_OUTPUT_NetCDF}/Masked/EBM_Model\")\n",
    "\n",
    "n_steps=4\n",
    "\n",
    "for reconstruction_time in all_times[:1]:\n",
    "    print(f\"Post Processing Time {reconstruction_time} Ma\")\n",
    "    # subduction_df = PK.get_subductiondf(reconstruction_time,tessellation_threshold_deg=0.7)\n",
    "    # subduction_lines=[]\n",
    "    # normal_angle=[]\n",
    "    \n",
    "    # for i in range(0, len(subduction_df)-1):\n",
    "    #     Point1=(subduction_df.iloc[i]['Trench Longitude'],subduction_df.iloc[i]['Trench Latitude'])\n",
    "    #     Point2=(subduction_df.iloc[i+1]['Trench Longitude'],subduction_df.iloc[i+1]['Trench Latitude'])\n",
    "    #     lines = LineString([Point1, Point2])\n",
    "    #     dist=lines.length\n",
    "    #     # if  subduction_df.iloc[i]['Trench Plate ID']== subduction_df.iloc[i+1]['Trench Plate ID'] and dist<50.0:\n",
    "    #     if  dist<=5.0:\n",
    "    #         subduction_lines.append(lines)\n",
    "    #     # else:\n",
    "    #     #     subduction_lines.append(np.nan)\n",
    "            \n",
    "        \n",
    "    # profiles=[]\n",
    "    # trench_lats=[]\n",
    "    # trench_lons=[]\n",
    "\n",
    "    # for i in range(0, len(subduction_df)-1):\n",
    "    #     dlon1 = n_steps * np.sin(np.radians(subduction_df.iloc[i]['Subduction Normal Angle']))\n",
    "    #     dlat1 = n_steps * np.cos(np.radians(subduction_df.iloc[i]['Subduction Normal Angle']))\n",
    "    #     x1=subduction_df.iloc[i]['Trench Longitude']\n",
    "    #     y1=subduction_df.iloc[i]['Trench Latitude']\n",
    "    #     ilon1 = x1 - dlon1\n",
    "    #     ilat1 = y1- dlat1\n",
    "        \n",
    "        \n",
    "    #     start_point = Point(x1- 0.01*dlon1, y1- 0.01*dlat1)\n",
    "    #     end_point = Point(ilon1, ilat1)\n",
    "    #     profile = LineString([start_point, end_point])\n",
    "    #     profiles.append(profile)\n",
    "    #     trench_lats.append(y1)\n",
    "    #     trench_lons.append(x1) \n",
    "        \n",
    "    # intersected_points=[]\n",
    "    # intersected_profiles=[]\n",
    "    # intersected_trench_lats=[]\n",
    "    # intersected_trench_lons=[]\n",
    "    # for i in range(len(profiles)):\n",
    "    #     for subduction_line in subduction_lines:\n",
    "    #         # try:\n",
    "    #         intersection=profiles[i].intersection(subduction_line)\n",
    "    #         if intersection.geom_type == 'Point':\n",
    "    #             # print(profiles[i])\n",
    "    #             intersected_profiles.append(profiles[i])\n",
    "    #             intersected_trench_lats.append(trench_lats[i])\n",
    "    #             intersected_trench_lons.append(trench_lons[i])\n",
    "    #             intersected_points.append(intersection)\n",
    "    #             # intersections2.append(intersection)\n",
    "                \n",
    "    # Intersection_df=gpd.GeoDataFrame()\n",
    "    # Intersection_df['Trench Latitude I']=intersected_trench_lats\n",
    "    # Intersection_df['Trench Longitude I']=intersected_trench_lons\n",
    "    # Intersection_df['Intersection Point']=intersected_points\n",
    "    # Intersection_df['Intersection Profile']=intersected_profiles\n",
    "    # Intersection_df['Intersection Latitude']=Intersection_df['Intersection Point'].apply(lambda p: p.y)\n",
    "    # Intersection_df['Intersection Longitude']=Intersection_df['Intersection Point'].apply(lambda p: p.x)\n",
    "    # Intersection_df=Intersection_df.set_geometry(gpd.points_from_xy(Intersection_df['Trench Longitude I'],Intersection_df['Trench Latitude I']))\n",
    "    # # Perform the spatial join\n",
    "    # result = gpd.sjoin_nearest(subduction_df, Intersection_df, how='left', max_distance=1.0)\n",
    "    # result=result.reset_index()\n",
    "    # Drop rows with the same index, keeping only the first occurrence\n",
    "    # result = result.drop_duplicates(subset=['index'], keep='first')\n",
    "    # result=result.set_index('index')\n",
    "    # # Alternatively, drop duplicates based on the index column\n",
    "    # # If you want to drop duplicates based on specific columns, replace 'index_left' with the appropriate column names\n",
    "    \n",
    "    # result ['Intersection Latitude'] = result ['Intersection Latitude'].fillna(result['Trench Latitude'] -n_steps*np.cos(np.radians(result['Subduction Normal Angle'])))\n",
    "    # result ['Intersection Longitude'] = result ['Intersection Longitude'].fillna(result['Trench Longitude'] -n_steps*np.sin(np.radians(result['Subduction Normal Angle'])))\n",
    "    \n",
    "    \n",
    "    # polygons=[]\n",
    "\n",
    "    # results = Parallel(n_jobs=-1)(\n",
    "    #     delayed(poly_around_sub)(i, result,resolution=0.1) for i in range(len(result) - 1)\n",
    "    # )\n",
    "    \n",
    "    # for i, res in enumerate(results):\n",
    "    #     try:\n",
    "    #         polygons.extend(res['polygon'])\n",
    "\n",
    "    #     except Exception as e:\n",
    "    #         # print(e)\n",
    "    #         pass\n",
    "    \n",
    "    Data=pd.read_parquet(f'{DEFAULT_OUTPUT_CSV}/Prediction/Predicted_{MODEL_NAME}_{reconstruction_time}Ma.parquet')\n",
    "    age_grid_file = find_filename_with_number(PK.agegrid,reconstruction_time)\n",
    "    print(age_grid_file)\n",
    "    oceanic_crust = rasterio.open(age_grid_file)\n",
    "    coordinates = [(x, y) for x, y in zip(Data['Longitude'].values, Data['Latitude'].values)]\n",
    "    oceanic_crust_point = list(oceanic_crust.sample(coordinates))\n",
    "    oceanic_crust_point=[oceanic_crust_point[i][0] for i in range(len(oceanic_crust_point))]\n",
    "    Data['Oceanic Crust']=oceanic_crust_point\n",
    "    Data = Data[pd.isna(Data['Oceanic Crust'])]\n",
    "    # Data_gdf=gpd.GeoDataFrame(Data,geometry=gpd.points_from_xy(Data['Longitude'],Data['Latitude']))\n",
    "    # Data_gdf=Data_gdf.set_crs(\"epsg:4326\")\n",
    "    # polygons_gdf=gpd.GeoDataFrame(geometry=polygons)\n",
    "    # polygons_gdf=polygons_gdf.set_crs(\"epsg:4326\")\n",
    "\n",
    "    # # Ensure the coordinate reference systems match\n",
    "    # if Data_gdf.crs != polygons_gdf.crs:\n",
    "    #     Data_gdf = Data_gdf.to_crs(polygons_gdf.crs)\n",
    "    # if 'index_right' in Data_gdf.columns:\n",
    "    #     Data_gdf=Data_gdf.drop(columns=['index_right'])\n",
    "    # # Perform spatial join to find points within polygons\n",
    "    # points_within_polygons = gpd.sjoin(Data_gdf, polygons_gdf, predicate='within')\n",
    "    \n",
    "    # # Remove points that are within polygons from the original points GeoDataFrame\n",
    "    # points_outside_polygons = Data_gdf.loc[~Data_gdf.index.isin(points_within_polygons.index)]\n",
    "    # Data= points_outside_polygons\n",
    "    \n",
    "    \n",
    "    \n",
    "    column_for_netcdf=\"ElevationRF\"\n",
    "    \n",
    "    da = df_to_NetCDF(x=Data['Longitude'], y=Data['Latitude'], z=Data[column_for_netcdf], statistic='mean', grid_resolution=0.1, clip=(None, None))\n",
    "    \n",
    "    raster=Raster(data=da, plate_reconstruction=PK.model, extent='global',  time=reconstruction_time)\n",
    "    raster.rotate_reference_frames(grid_spacing_degrees=NETCDF_GRID_RESOLUTION,\n",
    "                                   reconstruction_time=reconstruction_time, \n",
    "                                   from_rotation_features_or_model=PK.rotation_model, \n",
    "                                   to_rotation_features_or_model=PK.rotation_model, \n",
    "                                   from_rotation_reference_plate=Mantle_ID, \n",
    "                                   to_rotation_reference_plate=Paleomag_ID, \n",
    "                                   non_reference_plate=701, \n",
    "                                   output_name=f'{DEFAULT_OUTPUT_NetCDF}/Masked/RF_Model/RF_Model_{MODEL_NAME}_{reconstruction_time}.nc')\n",
    "\n",
    "\n",
    "\n",
    "    column_for_netcdf=\"ElevationDL\"\n",
    "    \n",
    "    da = df_to_NetCDF(x=Data['Longitude'], y=Data['Latitude'], z=Data[column_for_netcdf], statistic='mean', grid_resolution=0.1, clip=(None, None))\n",
    "    \n",
    "    raster=Raster(data=da, plate_reconstruction=PK.model, extent='global',  time=reconstruction_time)\n",
    "    raster.rotate_reference_frames(grid_spacing_degrees=NETCDF_GRID_RESOLUTION,\n",
    "                                   reconstruction_time=reconstruction_time, \n",
    "                                   from_rotation_features_or_model=PK.rotation_model, \n",
    "                                   to_rotation_features_or_model=PK.rotation_model, \n",
    "                                   from_rotation_reference_plate=Mantle_ID, \n",
    "                                   to_rotation_reference_plate=Paleomag_ID, \n",
    "                                   non_reference_plate=701, \n",
    "                                   output_name=f'{DEFAULT_OUTPUT_NetCDF}/Masked/DL_Model//DL_Model_{MODEL_NAME}_{reconstruction_time}.nc')\n",
    "\n",
    "\n",
    "    column_for_netcdf=\"ElevationDLC\"\n",
    "    \n",
    "    da = df_to_NetCDF(x=Data['Longitude'], y=Data['Latitude'], z=Data[column_for_netcdf], statistic='mean', grid_resolution=0.1, clip=(None, None))\n",
    "    \n",
    "    raster=Raster(data=da, plate_reconstruction=PK.model, extent='global',  time=reconstruction_time)\n",
    "    raster.rotate_reference_frames(grid_spacing_degrees=NETCDF_GRID_RESOLUTION,\n",
    "                                   reconstruction_time=reconstruction_time, \n",
    "                                   from_rotation_features_or_model=PK.rotation_model, \n",
    "                                   to_rotation_features_or_model=PK.rotation_model, \n",
    "                                   from_rotation_reference_plate=Mantle_ID, \n",
    "                                   to_rotation_reference_plate=Paleomag_ID, \n",
    "                                   non_reference_plate=701, \n",
    "                                   output_name=f'{DEFAULT_OUTPUT_NetCDF}/Masked/DLC_Model/DLC_Model_{MODEL_NAME}_{reconstruction_time}.nc')\n",
    "\n",
    "\n",
    "\n",
    "    column_for_netcdf=\"Paleotopography\"\n",
    "    \n",
    "    da = df_to_NetCDF(x=Data['Longitude'], y=Data['Latitude'], z=Data[column_for_netcdf], statistic='mean', grid_resolution=0.1, clip=(None, None))\n",
    "    \n",
    "    raster=Raster(data=da, plate_reconstruction=PK.model, extent='global',  time=reconstruction_time)\n",
    "    raster.rotate_reference_frames(grid_spacing_degrees=NETCDF_GRID_RESOLUTION,\n",
    "                                   reconstruction_time=reconstruction_time, \n",
    "                                   from_rotation_features_or_model=PK.rotation_model, \n",
    "                                   to_rotation_features_or_model=PK.rotation_model, \n",
    "                                   from_rotation_reference_plate=Mantle_ID, \n",
    "                                   to_rotation_reference_plate=Paleomag_ID, \n",
    "                                   non_reference_plate=701, \n",
    "                                   output_name=f'{DEFAULT_OUTPUT_NetCDF}/Masked/Ensemble_Model/Ensemble_Model_{MODEL_NAME}_{reconstruction_time}.nc')\n",
    "\n",
    "\n",
    "    column_for_netcdf2=\"ElevationEBM\"\n",
    "   \n",
    "    if compression:\n",
    "        encoding = {column_for_netcdf2: compression}\n",
    "    else:\n",
    "        encoding = None\n",
    "    \n",
    "    da = df_to_NetCDF(x=Data['Longitude'], y=Data['Latitude'], z=Data[column_for_netcdf2], statistic='mean', grid_resolution=0.1, clip=(None, None))\n",
    "    raster=Raster(data=da, plate_reconstruction=PK.model, extent='global',  time=reconstruction_time)\n",
    "    raster.rotate_reference_frames(grid_spacing_degrees=NETCDF_GRID_RESOLUTION,\n",
    "                                   reconstruction_time=reconstruction_time, \n",
    "                                   from_rotation_features_or_model=PK.rotation_model, \n",
    "                                   to_rotation_features_or_model=PK.rotation_model, \n",
    "                                   from_rotation_reference_plate=Mantle_ID, \n",
    "                                   to_rotation_reference_plate=Paleomag_ID, \n",
    "                                   non_reference_plate=701, \n",
    "                                   output_name=f'{DEFAULT_OUTPUT_NetCDF}/Masked/EBM_Model/EBM_Model_{MODEL_NAME}_{reconstruction_time}.nc')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b04daa1-07e2-41a1-9498-ec6e71bb15cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "def create_directory_if_not_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def interpolate_and_save_as_nc(start_time, end_time, required_time_step=1):\n",
    "    initial_time_step = int(end_time - start_time)\n",
    "    \n",
    "    try:\n",
    "        # Open the NetCDF files with xarray\n",
    "        src_start = xr.open_dataset(f\"/Users/ssin4735/Documents/PROJECT/PhD Project/Codes and Data/DeepTimeTopo/Workflow/Paleotopography/NetCDF/RF/RF_Model_phase2_{start_time}.nc\")\n",
    "        src_end = xr.open_dataset(f\"/Users/ssin4735/Documents/PROJECT/PhD Project/Codes and Data/DeepTimeTopo/Workflow/Paleotopography/NetCDF/RF/RF_Model_phase2_{end_time}.nc\")\n",
    "\n",
    "        # Get the data variable name (replace 'ElevationRF (Ca)' with the actual data variable name if different)\n",
    "        data_variable_name = 'Paleotopography'  # Ensure this matches the variable name in your files\n",
    "        start_data = src_start[data_variable_name]\n",
    "        end_data = src_end[data_variable_name]\n",
    "\n",
    "        # Extract lat/lon variables\n",
    "        latitudes = src_start['Latitude']\n",
    "        longitudes = src_start['Longitude']\n",
    "\n",
    "        # Define the interpolation time steps\n",
    "        time_steps = range(1, initial_time_step, required_time_step)\n",
    "\n",
    "        for time in time_steps:\n",
    "            # Calculate the interpolated data using your formula\n",
    "            time_fraction = time / float(initial_time_step)\n",
    "            interpolated_data = start_data + (end_data - start_data) * time_fraction\n",
    "\n",
    "            # Prepare NetCDF parameters\n",
    "            output_nc = f\"/Users/ssin4735/Documents/PROJECT/PhD Project/Codes and Data/DeepTimeTopo/Workflow/Paleotopography/NetCDF/RF/RF_Model_phase2_{start_time+time}.nc\"\n",
    "            create_directory_if_not_exists(os.path.dirname(output_nc))\n",
    "            \n",
    "            # Create a new xarray Dataset for the interpolated data\n",
    "            ds_interpolated = xr.Dataset(\n",
    "                {\n",
    "                    data_variable_name: (['Latitude', 'Longitude'], interpolated_data.values),\n",
    "                },\n",
    "                coords={\n",
    "                    'Latitude': latitudes,\n",
    "                    'Longitude': longitudes,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Copy attributes from source files\n",
    "            ds_interpolated.attrs.update(src_start.attrs)\n",
    "            ds_interpolated[data_variable_name].attrs.update(src_start[data_variable_name].attrs)\n",
    "            \n",
    "            # Save to NetCDF with compression\n",
    "            ds_interpolated.to_netcdf(output_nc, mode='w', engine='netcdf4', format='NETCDF4', encoding={data_variable_name: {'zlib': True, 'complevel': 5}})\n",
    "            \n",
    "        print(f\"Interpolation completed from {start_time} Ma to {end_time} Ma.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"No raster file for interpolating from {start_time} Ma to {end_time} Ma: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "# i = 10\n",
    "# all_times = [13, 14, 15]  # Example times, update with your actual time steps\n",
    "# interpolate_and_save_as_nc(start_time=all_times[i], end_time=all_times[i+1], required_time_step=1)\n",
    "all_times=glob.glob(\"/Users/ssin4735/Documents/PROJECT/PhD Project/Codes and Data/DeepTimeTopo/Workflow/Paleotopography//NetCDF/RF/RF_Model_phase2_*.nc\")\n",
    "\n",
    "# all_times=glob.glob(f\"{DEFAULT_OUTPUT_CSV}/Processed_WMA3/*\")\n",
    "all_times=np.sort([int(time.split('_')[-1].split('.')[0]) for time in all_times])\n",
    "for i in range(len(all_times)-1):\n",
    "    interpolate_and_save_as_nc(start_time=all_times[i], end_time=all_times[i+1], required_time_step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d51246-6633-48f2-ac0f-4ef4f6cc9060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
